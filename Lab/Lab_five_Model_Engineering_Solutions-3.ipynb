{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.4"},"colab":{"name":"Lab_five_Model_Engineering_Solutions.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"pycharm":{"stem_cell":{"cell_type":"raw","source":[],"metadata":{"collapsed":false}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"wazQdghw1h2_"},"source":["<center>\n","    <h3>University of Toronto</h3>\n","    <h3>Department of Mechanical and Industrial Engineering</h3>\n","    <h3>MIE368 Analytics in Action </h3>\n","    <h3>(Fall 2020)</h3>\n","    <hr>\n","    <h1>Lab 5: Model Engineering</h1>\n","    <h3>October 28, 2020</h3>\n","</center>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"75iHeAGv1h3C"},"source":["# Introduction\n","\n","We have implemented several different models throughout this course and  discussed when one is more appropriate than the other. However, often times in practice there are no clear guidelines on how to develop the best model for an application. Suppose we want to build a binary classifier with high recall and precision. How should we proceed if our off-the-shelf model performs poorly on the dataset?\n","\n","In this lab, we will explore a series of techniques for improving your model and features. We will cover seven techniques:\n","\n","- **Model selection**: We can try several different models to see what works best.\n","- **Scaling**: The scale of the values in your data is important\n","- **Feature engineering**: If the existing features aren't useful then we can create new and more relevant features   \n","- **Feature section**: If you have too many features then we can remove some that aren't useful\n","- **Grid Search**: a process for finding a set of good hyper-parameters (e.g., regularization strength)\n","- **Model stacking**: a process where the output from one or more models are input into another.\n","- **Bagging**: a process for combining models that involves combining the outputs from multiple models.\n","\n","Model engineering is more of an art than it is a science. These techniques will work better in some instances than they do in others, and sometimes these techniques can actually make a model perform worse! When you do model engineering it really boils does to doing trial-and-error with these techniques. It is possible to combine these techniques (e.g., do model selection, scaling, and grid search), however, to stop this lab from becoming convoluted you will only combine each technique with model selection.\n","\n","Please keep in mind that these are just a set of possible ways to improve our models. There are many ways to engineer better models, and often the practice of model engineering comes down to the application and our prior experience. We emphasis that in this lab we will largely apply these techniques in isolation, but we encourage you to combine multiple techniques in your project."]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"Ne5Do4xAIue5"},"source":["## Application\n","This lab will focus on the lending industry, where investors provide loans to borrowers in exchange for the promise of repayment with interest. If the borrower repays the loan, then the lender profits from the interest. However, sometimes the borrower is unable to repay the loan, meaning the lender loses money. The lender in this problem wants to predict if a borrower is unlikely to repay a loan and has asked us to build the best possible prediction model.\n","\n","First, let us import the essential tools."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"hw1OAVAnIue6","executionInfo":{"status":"ok","timestamp":1604262287960,"user_tz":300,"elapsed":469,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}}},"source":["# Standard analytics packages\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from sklearn.cluster import KMeans\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.feature_selection import SelectKBest, f_classif\n","# Import models\n","from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n","# Model evaluation \n","from sklearn.metrics import precision_score, recall_score\n","from sklearn.model_selection import GridSearchCV\n","# Import model engineering\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.tree import DecisionTreeClassifier"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"Q-kQo0hEIue-"},"source":["Next, we'll load the dataset for this lab and store it as `df_raw`."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"cLKPAHPZIue-","executionInfo":{"status":"ok","timestamp":1604262288991,"user_tz":300,"elapsed":1486,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"94a21695-ad5f-4a48-a7d2-6466461295a1","colab":{"base_uri":"https://localhost:8080/","height":215}},"source":["# Import data for lab\n","df_raw = pd.read_csv(\"https://docs.google.com/uc?export=download&id=1zj7qSc5mjgXLjQgOfF60PsbO2wupLnTm\")\n","df_raw.head()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CreditPolicy</th>\n","      <th>Purpose</th>\n","      <th>IntRate</th>\n","      <th>Installment</th>\n","      <th>Dti</th>\n","      <th>Fico</th>\n","      <th>DaysWithCrLine</th>\n","      <th>RevolBal</th>\n","      <th>RevolUtil</th>\n","      <th>InqLast6mths</th>\n","      <th>Delinq2yrs</th>\n","      <th>PubRec</th>\n","      <th>NotFullyPaid</th>\n","      <th>AnnualInc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>debt_consolidation</td>\n","      <td>0.1189</td>\n","      <td>829.10</td>\n","      <td>19.48</td>\n","      <td>737</td>\n","      <td>5639.958333</td>\n","      <td>28854</td>\n","      <td>52.1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>85000.000385</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>credit_card</td>\n","      <td>0.1071</td>\n","      <td>228.22</td>\n","      <td>14.29</td>\n","      <td>707</td>\n","      <td>2760.000000</td>\n","      <td>33623</td>\n","      <td>76.7</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>65000.000073</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>debt_consolidation</td>\n","      <td>0.1357</td>\n","      <td>366.86</td>\n","      <td>11.63</td>\n","      <td>682</td>\n","      <td>4710.000000</td>\n","      <td>3511</td>\n","      <td>25.6</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>31999.999943</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>debt_consolidation</td>\n","      <td>0.1008</td>\n","      <td>162.34</td>\n","      <td>8.10</td>\n","      <td>712</td>\n","      <td>2699.958333</td>\n","      <td>33667</td>\n","      <td>73.2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>85000.000385</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>credit_card</td>\n","      <td>0.1426</td>\n","      <td>102.92</td>\n","      <td>14.97</td>\n","      <td>667</td>\n","      <td>4066.000000</td>\n","      <td>4740</td>\n","      <td>39.5</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>80799.999636</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   CreditPolicy             Purpose  ...  NotFullyPaid     AnnualInc\n","0             1  debt_consolidation  ...             0  85000.000385\n","1             1         credit_card  ...             0  65000.000073\n","2             1  debt_consolidation  ...             0  31999.999943\n","3             1  debt_consolidation  ...             0  85000.000385\n","4             1         credit_card  ...             0  80799.999636\n","\n","[5 rows x 14 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"VMKE0_qiIufB"},"source":["The data if `df_raw` is described in the following data dictionary:\n","\n","|Columns         |Definition                               |\n","|:--------------:|:----------------------------------------|\n","|**Features**    |  |\n","|CreditPolicy    |1 if the borrower meets the underwriting criteria                       |\n","|Purpose         |Written purpose of the loan                           |    \n","|IntRate         |Assigned interest rate                           |\n","|Installment     |Monthly installments                      |  \n","|Dti             |Borrower's debt to income ratio               |\n","|Fico            |Borrower's FICO (credit) score                    |\n","|DaysWithCrLine  |Number of days the borrower has had an existing line of credit                 |\n","|RevolBal        |Revolving balance (current unpaid credit-card debt)            |\n","|RevolUtil       |Revolving line utilization (fraction of line of credit used)                 |\n","|InqLast5mths    |Number of inquiries by creditors in last 5 months            |\n","|Delinq2yrs      |Number of times the borrower has been 30+days past due in last 2 years                         |\n","|PubReq          |Number of derogatory public records                    |\n","|AnnualInc       |Self-reported annual income of borrower         |\n","|**Target**      |                                 |\n","|NotFullyPaid    |1 if the loan was not fully paid back                       |\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"pasUct3AIufC"},"source":["# Exploratory data analysis and cleaning\n","In this section, we'll quickly clean the data so that all categorical information is encoded as a series of binary values and then we'll do some brief exploratory data analysis (EDA) to better understand the data. "]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"DNb8GQJAIufC"},"source":["## Data Cleaning\n","Note that the dataset is already quite clean (e.g., no missing values). The only real issue with it right now is that the \"Purpose\" column in `df_raw` is categorical. Let's encode the information into a series of binary variables. Note, that encoding values as zeros and ones is often called _one-hot encoding_ in the anaytics community, however, it also goes by _making dummy variables_ in the statistics community. We will use the term one-hot encoding to be consistent with the analytics community, however, `pandas` (and some other popular packages) use the term \"dummies\". "]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"8Eprz59_IufD","executionInfo":{"status":"ok","timestamp":1604262288993,"user_tz":300,"elapsed":1475,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"0355b299-2248-4630-8191-6e46e909100c","colab":{"base_uri":"https://localhost:8080/","height":215}},"source":["'''Convert categorical data into \"dummy\" variables'''\n","df = pd.get_dummies(df_raw, columns=['Purpose']) \n","df.head()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CreditPolicy</th>\n","      <th>IntRate</th>\n","      <th>Installment</th>\n","      <th>Dti</th>\n","      <th>Fico</th>\n","      <th>DaysWithCrLine</th>\n","      <th>RevolBal</th>\n","      <th>RevolUtil</th>\n","      <th>InqLast6mths</th>\n","      <th>Delinq2yrs</th>\n","      <th>PubRec</th>\n","      <th>NotFullyPaid</th>\n","      <th>AnnualInc</th>\n","      <th>Purpose_all_other</th>\n","      <th>Purpose_credit_card</th>\n","      <th>Purpose_debt_consolidation</th>\n","      <th>Purpose_educational</th>\n","      <th>Purpose_home_improvement</th>\n","      <th>Purpose_major_purchase</th>\n","      <th>Purpose_small_business</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0.1189</td>\n","      <td>829.10</td>\n","      <td>19.48</td>\n","      <td>737</td>\n","      <td>5639.958333</td>\n","      <td>28854</td>\n","      <td>52.1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>85000.000385</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.1071</td>\n","      <td>228.22</td>\n","      <td>14.29</td>\n","      <td>707</td>\n","      <td>2760.000000</td>\n","      <td>33623</td>\n","      <td>76.7</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>65000.000073</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0.1357</td>\n","      <td>366.86</td>\n","      <td>11.63</td>\n","      <td>682</td>\n","      <td>4710.000000</td>\n","      <td>3511</td>\n","      <td>25.6</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>31999.999943</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0.1008</td>\n","      <td>162.34</td>\n","      <td>8.10</td>\n","      <td>712</td>\n","      <td>2699.958333</td>\n","      <td>33667</td>\n","      <td>73.2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>85000.000385</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0.1426</td>\n","      <td>102.92</td>\n","      <td>14.97</td>\n","      <td>667</td>\n","      <td>4066.000000</td>\n","      <td>4740</td>\n","      <td>39.5</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>80799.999636</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   CreditPolicy  IntRate  ...  Purpose_major_purchase  Purpose_small_business\n","0             1   0.1189  ...                       0                       0\n","1             1   0.1071  ...                       0                       0\n","2             1   0.1357  ...                       0                       0\n","3             1   0.1008  ...                       0                       0\n","4             1   0.1426  ...                       0                       0\n","\n","[5 rows x 20 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"wVioAlyCIufF"},"source":["Throughout this lab we use a training set (i.e., `X_train` and `y_train`) and validation set (e.g., `X_val` and `y_val`).\n"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"I7jVeR-fIufG","executionInfo":{"status":"ok","timestamp":1604262288993,"user_tz":300,"elapsed":1473,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}}},"source":["'''Make training and validation dataset that we'll use throughout the lab'''\n","# Split the data\n","X_train, X_val, y_train, y_val = train_test_split(df.drop(columns=['NotFullyPaid']),\n","                                                    df.NotFullyPaid,\n","                                                    test_size = 0.3,\n","                                                    random_state = 1)\n","\n","\n","# Make a dataframe for the training data \n","df_train = X_train.join(y_train)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"qc6LRDsOIufI"},"source":["We can analyze `X_train` and `y_train` as much as we want when designing or optimizing a model. However, you should avoid doing EDA on your validation or testing set (i.e., out-of-sample data) to avoid _data leakage_. Data leakage occurs when information from your out-of-sample data is incorporated into your models (often unintentionally). In general, you perform EDA to guide your modeling decisions, so if your out-of-sample data is included in your EDA then it may influence your modeling decisions."]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"TQJoUyNjIufJ"},"source":["## EDA\n","Use the data frame `df_train` to do all your EDA to ensure their is no data leakage. \n","\n","We can use the `describe()` method on a data frame to print out a summary of the columns. Use the output of `describe()` or write your own code where appropriate to answer the following questions."]},{"cell_type":"markdown","metadata":{"id":"jFMF3ePiA4sU"},"source":["### Exercises\n","\n","1. Apply the `describe()` method to `df_train`."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"BW4eo_IxIufJ","executionInfo":{"status":"ok","timestamp":1604262289117,"user_tz":300,"elapsed":1585,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"3b7a1f01-8cc0-46e1-ec97-2df99f2b70bd","colab":{"base_uri":"https://localhost:8080/","height":304}},"source":["# Write your code here.  \n","# -------------------\n","\n","df_train_describe = df_train.describe()\n","df_train_describe\n","\n","# -------------------"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CreditPolicy</th>\n","      <th>IntRate</th>\n","      <th>Installment</th>\n","      <th>Dti</th>\n","      <th>Fico</th>\n","      <th>DaysWithCrLine</th>\n","      <th>RevolBal</th>\n","      <th>RevolUtil</th>\n","      <th>InqLast6mths</th>\n","      <th>Delinq2yrs</th>\n","      <th>PubRec</th>\n","      <th>AnnualInc</th>\n","      <th>Purpose_all_other</th>\n","      <th>Purpose_credit_card</th>\n","      <th>Purpose_debt_consolidation</th>\n","      <th>Purpose_educational</th>\n","      <th>Purpose_home_improvement</th>\n","      <th>Purpose_major_purchase</th>\n","      <th>Purpose_small_business</th>\n","      <th>NotFullyPaid</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6.704000e+03</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6.704000e+03</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","      <td>6704.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.801014</td>\n","      <td>0.122590</td>\n","      <td>319.644112</td>\n","      <td>12.609300</td>\n","      <td>710.907369</td>\n","      <td>4557.635889</td>\n","      <td>1.694455e+04</td>\n","      <td>46.708816</td>\n","      <td>1.602327</td>\n","      <td>0.159755</td>\n","      <td>0.062351</td>\n","      <td>6.832708e+04</td>\n","      <td>0.243586</td>\n","      <td>0.131265</td>\n","      <td>0.410949</td>\n","      <td>0.037739</td>\n","      <td>0.065185</td>\n","      <td>0.045943</td>\n","      <td>0.065334</td>\n","      <td>0.157518</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.399267</td>\n","      <td>0.027047</td>\n","      <td>206.869072</td>\n","      <td>6.882637</td>\n","      <td>38.010970</td>\n","      <td>2501.947628</td>\n","      <td>3.532676e+04</td>\n","      <td>29.055547</td>\n","      <td>2.184938</td>\n","      <td>0.524771</td>\n","      <td>0.262517</td>\n","      <td>6.229470e+04</td>\n","      <td>0.429278</td>\n","      <td>0.337715</td>\n","      <td>0.492043</td>\n","      <td>0.190578</td>\n","      <td>0.246870</td>\n","      <td>0.209377</td>\n","      <td>0.247133</td>\n","      <td>0.364316</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.060000</td>\n","      <td>15.670000</td>\n","      <td>0.000000</td>\n","      <td>612.000000</td>\n","      <td>180.041667</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.896000e+03</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>1.000000</td>\n","      <td>0.103300</td>\n","      <td>164.020000</td>\n","      <td>7.250000</td>\n","      <td>682.000000</td>\n","      <td>2819.958333</td>\n","      <td>3.152250e+03</td>\n","      <td>22.600000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>3.840000e+04</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>1.000000</td>\n","      <td>0.122100</td>\n","      <td>268.530000</td>\n","      <td>12.600000</td>\n","      <td>707.000000</td>\n","      <td>4110.041667</td>\n","      <td>8.546500e+03</td>\n","      <td>46.100000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>5.518800e+04</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>1.000000</td>\n","      <td>0.141100</td>\n","      <td>434.867500</td>\n","      <td>17.940000</td>\n","      <td>737.000000</td>\n","      <td>5730.041667</td>\n","      <td>1.808175e+04</td>\n","      <td>70.700000</td>\n","      <td>2.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>8.000000e+04</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1.000000</td>\n","      <td>0.216400</td>\n","      <td>922.420000</td>\n","      <td>29.950000</td>\n","      <td>827.000000</td>\n","      <td>17616.000000</td>\n","      <td>1.207359e+06</td>\n","      <td>119.000000</td>\n","      <td>33.000000</td>\n","      <td>11.000000</td>\n","      <td>4.000000</td>\n","      <td>2.039784e+06</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       CreditPolicy      IntRate  ...  Purpose_small_business  NotFullyPaid\n","count   6704.000000  6704.000000  ...             6704.000000   6704.000000\n","mean       0.801014     0.122590  ...                0.065334      0.157518\n","std        0.399267     0.027047  ...                0.247133      0.364316\n","min        0.000000     0.060000  ...                0.000000      0.000000\n","25%        1.000000     0.103300  ...                0.000000      0.000000\n","50%        1.000000     0.122100  ...                0.000000      0.000000\n","75%        1.000000     0.141100  ...                0.000000      0.000000\n","max        1.000000     0.216400  ...                1.000000      1.000000\n","\n","[8 rows x 20 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"Qh2ClVg8IufM"},"source":["2. What proportion of borrows default on their loans?"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"iY8GkdalIufN","executionInfo":{"status":"ok","timestamp":1604262289118,"user_tz":300,"elapsed":1576,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"2712670d-7174-49b5-eff5-cee8cc581721","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Write your code here.  \n","# -------------------\n","\n","df_train_describe.NotFullyPaid.loc['mean']\n","\n","# -------------------"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.1575178997613365"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"6ZJcH9gyIufQ"},"source":["3. Which of the purposes for borrowing is most likely to lead to a default?"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"ETdlFvRJIufQ","executionInfo":{"status":"ok","timestamp":1604262289118,"user_tz":300,"elapsed":1564,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"6de55803-d267-4b2d-f683-33bf3395d9e3","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Write your code here.  \n","# -------------------\n","\n","summary_df = df_train.groupby('NotFullyPaid').sum()\n","summary_fraction_df = summary_df.loc[1]/summary_df.loc[0]\n","summary_fraction_df.idxmax()\n","\n","# -------------------"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Purpose_small_business'"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"RA9AXQbdIufT"},"source":["4. Which of the purposes has the highest correlation with the `NotFullyPaid` variable?\n"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"KhJFB-8DIufT","executionInfo":{"status":"ok","timestamp":1604262289307,"user_tz":300,"elapsed":1742,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"ca8d354f-4701-4c5b-e064-befb61fbd690","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Write your code here.  \n","# -------------------\n","\n","correlation_vector = df_train.corr().NotFullyPaid.drop('NotFullyPaid')\n","correlation_vector.abs().idxmax()\n","# -------------------"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'CreditPolicy'"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"jkFC29XhIufY"},"source":["# Tracking progress\n","\n","Throughout this lab we make several models. In this section we will initialize a function `make_models` and a data frame `all_models` that we will use throughout the lab. "]},{"cell_type":"markdown","metadata":{"id":"rkuHhysLEXcN"},"source":["## Defining the `make_models` function\n","\n","We will apply the seven techniques in this lab using four different models, which are initialized in the `make_models` function below.\n","\n","* _LR_L2_: is a logistic regression with an L2 loss\n","* _LR_L1_: is a logistic regression with an L1 loss with \"balanced\" class weights\n","* _CART_: is a CART tree with with \"balanced\" class weights\n","* _RF_: is a random forest with with \"balanced\" class weights\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"AlT2vkAyEXs9","executionInfo":{"status":"ok","timestamp":1604262289309,"user_tz":300,"elapsed":1742,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}}},"source":["def make_models():\n","  \"\"\"Makes a dictionary of four untrained models\"\"\"\n","  \n","  return {\n","      'LR_L2': LogisticRegression(random_state=0),\n","      'LR_L1': LogisticRegression(random_state=0, penalty='l1', solver='liblinear', class_weight='balanced'),\n","      'CART': DecisionTreeClassifier(random_state=0, class_weight='balanced'),\n","      'RF': RandomForestClassifier(random_state=0, class_weight='balanced'),\n","  }\n"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hjsTfAlPMHpT"},"source":["### Exercise\n","\n","> 1. Notice that we set the parameter `class_weights` to `'balanced'`. Look up the documentation for `LogisticRegression`. What does the `class_weights = 'balanced'` do? Why do you think its appropriate here?\n",">> \"balanced\" class weights will adjust the weights of the target values to make them inversely proportional to class frequencies in the training data as n_samples / (n_classes * np.bincount(y). For example, in this dataset where 16% of targets are 1, predicting each 0 correctly gets 0.16 the reward of predicting 1 correctly. This helps to fight the imbalance in the dataset"]},{"cell_type":"markdown","metadata":{"id":"T68pTHazEYks"},"source":["## Initializing the `all_models` data frame\n","\n","In the code block below we initialize the `all_models` data frame. You don't need to understand every line of code, the more important this is to understand that we have a data fram called `all_models` that we will use throughout this lab to track progress."]},{"cell_type":"code","metadata":{"id":"zYdNb-vGxAE7","executionInfo":{"status":"ok","timestamp":1604262289310,"user_tz":300,"elapsed":1730,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"10d3716e-d313-4808-cd8a-e09e3ec3ffe5","colab":{"base_uri":"https://localhost:8080/","height":906}},"source":["'''Create a data frame to keep track of all the models we train in this lab'''\n","\n","# Initialize a tuple of names for each model\n","model_names = ('LR_L2',  # Logistic regression with L2 regularizer\n","               'LR_L1',  # Logistic regression with L1 regularizer\n","               'CART',  # Cart tree classifier\n","               'RF'  # Random forest classifier \n","               )\n","# Initialize a tuple of techniqu names that we will cover\n","engineering_techniques = ('Baseline',  # Set of baseline models\n","                          'Scaling',  # Set of models trained with scaled data\n","                          'Feature Engineering',  # Set of models trained with engineered features\n","                          'Feature_Selection',  # Set of models trained with \"selected\" features\n","                          'Grid Search',  # Set of models trained via grid search\n","                          'Stacking',  # Set of stacked model \n","                          'Bagging'  # A bagged model\n","                          )\n","\n","# Initialize the multi indices of the `all_models` data frame\n","df_indices = pd.MultiIndex.from_product([model_names, engineering_techniques], names=('model names', 'technique'))\n","# Initialize the `all_models` data frame\n","all_models = pd.DataFrame(index=df_indices, columns=['Precision', 'Recall', 'Score', 'Model'])\n","all_models[['Precision', 'Recall', 'Score']] = all_models[['Precision', 'Recall', 'Score']].astype(float)\n","all_models  # Initialized data frame only has NaNs, which is perfect!"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Score</th>\n","      <th>Model</th>\n","    </tr>\n","    <tr>\n","      <th>model names</th>\n","      <th>technique</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"7\" valign=\"top\">LR_L2</th>\n","      <th>Baseline</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Scaling</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Feature Engineering</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Feature_Selection</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Grid Search</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Stacking</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Bagging</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"7\" valign=\"top\">LR_L1</th>\n","      <th>Baseline</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Scaling</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Feature Engineering</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Feature_Selection</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Grid Search</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Stacking</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Bagging</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"7\" valign=\"top\">CART</th>\n","      <th>Baseline</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Scaling</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Feature Engineering</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Feature_Selection</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Grid Search</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Stacking</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Bagging</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"7\" valign=\"top\">RF</th>\n","      <th>Baseline</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Scaling</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Feature Engineering</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Feature_Selection</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Grid Search</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Stacking</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Bagging</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 Precision  Recall  Score Model\n","model names technique                                          \n","LR_L2       Baseline                   NaN     NaN    NaN   NaN\n","            Scaling                    NaN     NaN    NaN   NaN\n","            Feature Engineering        NaN     NaN    NaN   NaN\n","            Feature_Selection          NaN     NaN    NaN   NaN\n","            Grid Search                NaN     NaN    NaN   NaN\n","            Stacking                   NaN     NaN    NaN   NaN\n","            Bagging                    NaN     NaN    NaN   NaN\n","LR_L1       Baseline                   NaN     NaN    NaN   NaN\n","            Scaling                    NaN     NaN    NaN   NaN\n","            Feature Engineering        NaN     NaN    NaN   NaN\n","            Feature_Selection          NaN     NaN    NaN   NaN\n","            Grid Search                NaN     NaN    NaN   NaN\n","            Stacking                   NaN     NaN    NaN   NaN\n","            Bagging                    NaN     NaN    NaN   NaN\n","CART        Baseline                   NaN     NaN    NaN   NaN\n","            Scaling                    NaN     NaN    NaN   NaN\n","            Feature Engineering        NaN     NaN    NaN   NaN\n","            Feature_Selection          NaN     NaN    NaN   NaN\n","            Grid Search                NaN     NaN    NaN   NaN\n","            Stacking                   NaN     NaN    NaN   NaN\n","            Bagging                    NaN     NaN    NaN   NaN\n","RF          Baseline                   NaN     NaN    NaN   NaN\n","            Scaling                    NaN     NaN    NaN   NaN\n","            Feature Engineering        NaN     NaN    NaN   NaN\n","            Feature_Selection          NaN     NaN    NaN   NaN\n","            Grid Search                NaN     NaN    NaN   NaN\n","            Stacking                   NaN     NaN    NaN   NaN\n","            Bagging                    NaN     NaN    NaN   NaN"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"KAw6taJkvu1N"},"source":["We're initialized a data frame `all_models`. Over the course of this lab, we will replace the NaNs in this data frame. "]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"UHqH0ApjIufb"},"source":["# Model selection (Baseline)\n","\n","For starters, we should fit our four baseline models. We will score each model using its average precision and recall (i.e., $\\frac{precision + recall}{2}$). We covered precision and recall in lab one, but you can refer to [this article](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall) for a refresher."]},{"cell_type":"markdown","metadata":{"id":"a-L_dM7TH-p1"},"source":["### Exercise\n","\n","> 1. Complete the function below, which fits multiple models on a training dataset, and evaluates its precision, recall, and the average of precision and recall on the out-of-sample data. You should use the `precision_score()` and `recall_score()` functions below to calculate those metrics."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"YL8LwfNAIufb","executionInfo":{"status":"ok","timestamp":1604262293165,"user_tz":300,"elapsed":5574,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"afb0364f-26b2-409d-b85d-ca133420d568","colab":{"base_uri":"https://localhost:8080/","height":316}},"source":["# from sklearn.metrics import precision_score, recall_score\n","\n","def fit_and_score_model(all_models, stage_name, X_train, X_out_of_sample, y_train, y_out_of_sample):\n","    \"\"\"Fits the models that are inialized by models_dict on the X_train and y_train\n","    data, and evalautes the model on the out-of-sample data X_out_of_sample and y_out_of_sample\"\"\"\n","    \n","    # Make a dictionary of models\n","    models_dict = make_models()\n","\n","    # Loop through each model in model_dict\n","    for model_name in models_dict:\n","        model = models_dict[model_name]\n","        \n","        '''Write some code to fit the model, and calculate precision (call it \n","        model_precision), recall (call it model_recall), and score (call it \n","        model_score) on the validation set.'''\n","        \n","\n","        # Write your code here.  \n","        # -------------------------------------------------------------------------\n","    \n","        model.fit(X_train, y_train)  # fit the model\n","    \n","        model_precision = precision_score(y_out_of_sample, model.predict(X_out_of_sample))  # evaluate precision on test set\n","        model_recall = recall_score(y_out_of_sample, model.predict(X_out_of_sample))  # evaluate recall on test set\n","        model_score = (model_precision + model_recall) / 2\n","        \n","        # -------------------------------------------------------------------------\n","        print(f'{model_name} achieved a precision of {model_precision:.3f} and recall of {model_recall:.3f}')\n","        \n","        all_models.loc[model_name, stage_name] = (model_precision, model_recall, model_score, model)\n","\n","    return all_models\n","    \n","all_models = fit_and_score_model(all_models, 'Baseline', X_train, X_val, y_train, y_val)\n","all_models.loc[:, 'Baseline', :].head()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"stream","text":["LR_L2 achieved a precision of 0.000 and recall of 0.000\n","LR_L1 achieved a precision of 0.257 and recall of 0.579\n","CART achieved a precision of 0.199 and recall of 0.203\n","RF achieved a precision of 0.167 and recall of 0.002\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Score</th>\n","      <th>Model</th>\n","    </tr>\n","    <tr>\n","      <th>model names</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>LR_L2</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n","    </tr>\n","    <tr>\n","      <th>LR_L1</th>\n","      <td>0.256506</td>\n","      <td>0.578616</td>\n","      <td>0.417561</td>\n","      <td>LogisticRegression(C=1.0, class_weight='balanc...</td>\n","    </tr>\n","    <tr>\n","      <th>CART</th>\n","      <td>0.199179</td>\n","      <td>0.203354</td>\n","      <td>0.201266</td>\n","      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n","    </tr>\n","    <tr>\n","      <th>RF</th>\n","      <td>0.166667</td>\n","      <td>0.002096</td>\n","      <td>0.084382</td>\n","      <td>(DecisionTreeClassifier(ccp_alpha=0.0, class_w...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             Precision  ...                                              Model\n","model names             ...                                                   \n","LR_L2         0.000000  ...  LogisticRegression(C=1.0, class_weight=None, d...\n","LR_L1         0.256506  ...  LogisticRegression(C=1.0, class_weight='balanc...\n","CART          0.199179  ...  DecisionTreeClassifier(ccp_alpha=0.0, class_we...\n","RF            0.166667  ...  (DecisionTreeClassifier(ccp_alpha=0.0, class_w...\n","\n","[4 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"cvjWh3sHT9HK"},"source":["Upon examining the `all_models` data frame, you should see that it keep tracks of the precision, recall, and a score (float types) in addition to the trained models (object type). You should also see that the baseline \"LR_L2\" model is very bad, but the other models looks a little more promising. "]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"4WXcJxNZIufe"},"source":["### Exercises\n","\n","\n","\n","> 1. We provided some code that will use the baseline LR_L2 model to predict target values. Describe those predictions. Does anything seem strange? (**Hint**: you should see an `UndefinedMetricWarning` after running the previous code block that mentions 'zero\\_division'.)"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"LRzsFIjDIufe","executionInfo":{"status":"ok","timestamp":1604262293166,"user_tz":300,"elapsed":5564,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"1a633ed3-a536-41ce-dc97-f0ecc1a3ee33","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Predict the target values using the logistic regression model\n","y_val_predictions = all_models.Model.loc['LR_L2','Baseline'].predict(X_val)\n","\n","# Write your code here\n","# -------------------------------------------------------------------------\n","\n","y_val_predictions.mean()\n","# All predictions are zero\n","\n","# -------------------------------------------------------------------------"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"y_mWwqWIIufh"},"source":["> 2. Why should we use precision and recall as our metrics for this problem rather than the usual ROC metrics?\n","\n",">> Precision and recall are better suited for imbalanced target data. Precision and recall can measure rare events much better than ROC metrics because they evaluate true positive, false positive, and false negative events, whereas ROC only looks at false positive and true positive. \n","\n","In the rest of this lab we will develop the best possible classifier to improve the score of each using one technique at a time (on these 4 baseline models). We will build and train our models on the training set and evaluate on the validation set."]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"nJskyDxcIufh"},"source":["# Data scaling\n","Next, lets scale the features to have a mean of zero and standard deviation of one (i.e., standardize). This is a very important step when features have different orders of magnitude. However, when features values have similar orders of magnitude (like they do now) you may see little to no improvement. The easiest way to standardize data is with the `StandardScaler` function, which is \"fit\" (i.e., calculates the mean and standard deviation) on the training set. Once trained, you should apply it to both the training and validation set before fitting and scoring your model. Note, that we can generally keep binary data ''as is''. Since our target data (whether or not a loan was repaid) is binary, we won't scale the target, which makes the data scaling step a little easier. However, if the target data was a continuous variable, then we could follow the same steps that we will go through with our feature data to scale the target data. "]},{"cell_type":"markdown","metadata":{"id":"JwK1RuQ7Mzkb"},"source":["### Exercise\n","\n","> 1. Complete the function `standardize_data` that initializes a `StandardScaler`, fits it on a training dataset (`X_train`). Use the fit `scaler` to to standardize (i.e., transform) the training and out-of-sample data (`X_val`), and call the variables `X_train_standarized` and `X_out_of_sample_standarized`, respectively.)"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"bvHvuBsrIufi","executionInfo":{"status":"ok","timestamp":1604262295411,"user_tz":300,"elapsed":7792,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"5562de21-ab7a-478d-dcb1-60c5a3759086","colab":{"base_uri":"https://localhost:8080/","height":262}},"source":["# from sklearn.preprocessing import StandardScaler\n","\n","technique_name = 'Scaling'\n","\n","def standardize_data(X_train, X_out_of_sample):\n","    \"\"\"Standardizes all of the data in X_train and X_out_of_sample. The mean and\n","    standard diviation of each feature (i.e., each column) from the X_train\n","    data is used to standarize both the X_train and X_out_of sample.\"\"\"\n","\n","    # Initialize data frame for scaled data\n","    X_train_standarized = X_train.copy()\n","    X_out_of_sample_standarized = X_out_of_sample.copy()\n","\n","    # Define scaling function\n","    scaler = StandardScaler()\n","    \n","    '''Use scaler to standardize your data. You'll need to fit scaler with your\n","    training data (use the fit method) and standardize your training and \n","    out-of-sample data (use the transform method)'''\n","    # -------------------------------------------------------------------------\n","\n","    # Scale the X features      \n","    scaler.fit(X_train)\n","    X_train_standarized.loc[:]  = scaler.transform(X_train)\n","    X_out_of_sample_standarized.loc[:] = scaler.transform(X_out_of_sample)\n","                        \n","    # -------------------------------------------------------------------------\n","\n","    return X_train_standarized, X_out_of_sample_standarized, scaler\n","\n","# Make new data that is scaled\"\n","X_train_scaled, X_val_scaled, scaler = standardize_data(X_train, X_val)\n","\n","# Fit and score a model trained with scaled data\n","all_models = fit_and_score_model(all_models, technique_name, X_train_scaled, X_val_scaled, y_train, y_val)\n","all_models.loc[:, technique_name, :].head()"],"execution_count":18,"outputs":[{"output_type":"stream","text":["LR_L2 achieved a precision of 0.471 and recall of 0.017\n","LR_L1 achieved a precision of 0.257 and recall of 0.587\n","CART achieved a precision of 0.200 and recall of 0.203\n","RF achieved a precision of 0.167 and recall of 0.002\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Score</th>\n","      <th>Model</th>\n","    </tr>\n","    <tr>\n","      <th>model names</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>LR_L2</th>\n","      <td>0.470588</td>\n","      <td>0.016771</td>\n","      <td>0.243680</td>\n","      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n","    </tr>\n","    <tr>\n","      <th>LR_L1</th>\n","      <td>0.257353</td>\n","      <td>0.587002</td>\n","      <td>0.422178</td>\n","      <td>LogisticRegression(C=1.0, class_weight='balanc...</td>\n","    </tr>\n","    <tr>\n","      <th>CART</th>\n","      <td>0.200000</td>\n","      <td>0.203354</td>\n","      <td>0.201677</td>\n","      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n","    </tr>\n","    <tr>\n","      <th>RF</th>\n","      <td>0.166667</td>\n","      <td>0.002096</td>\n","      <td>0.084382</td>\n","      <td>(DecisionTreeClassifier(ccp_alpha=0.0, class_w...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             Precision  ...                                              Model\n","model names             ...                                                   \n","LR_L2         0.470588  ...  LogisticRegression(C=1.0, class_weight=None, d...\n","LR_L1         0.257353  ...  LogisticRegression(C=1.0, class_weight='balanc...\n","CART          0.200000  ...  DecisionTreeClassifier(ccp_alpha=0.0, class_we...\n","RF            0.166667  ...  (DecisionTreeClassifier(ccp_alpha=0.0, class_w...\n","\n","[4 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"RoSM6ayT3Dxe"},"source":["## Defining the compare_models function\n","\n","Lets define the function `compare_models` to compare the models based on the technique `technique_name` to the \"Baseline\" models"]},{"cell_type":"code","metadata":{"id":"n0fJt13O3ED6","executionInfo":{"status":"ok","timestamp":1604262295412,"user_tz":300,"elapsed":7781,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"ed15e7c0-a2a8-4d11-b66a-5da3fef22c41","colab":{"base_uri":"https://localhost:8080/"}},"source":["def compare_models(technique_name):\n","  \"\"\"Prints out the average and biggest improvement observed between the \n","  models trained with technique_name and the Baseline models\"\"\"\n","\n","  # Evaluate score differences\n","  score_differences = (all_models.loc[:, technique_name, :].Score - all_models.loc[:, 'Baseline', :].Score)\n","\n","  # Get the average and biggest score improvement\n","  mean_score_difference = score_differences.mean()\n","  most_score_improvement = score_differences.max()\n","\n","  print(f'On average, scores improved by {mean_score_difference:.2f}, and the most improvement was {most_score_improvement:.2f}')\n","\n","compare_models(technique_name)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["On average, scores improved by 0.06, and the most improvement was 0.24\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"q-BFujZNIufk"},"source":["## Using a fit standardization method\n","If you want to convert your features (or target variables if they were scaled) back into the units from the original features, then you can use the `inverse_transform` method as we do below. Note, that the mean of the transformed training set should always be zero, but the mean of the out-of-sample set will likely just be close to zero because the standardization is based on the training set mean and standard deviation (using the testing set is cheating!). Below is just an example of how to run a quick sanity check to ensure the scaling is working properly."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"gcnjHmhQIufk","executionInfo":{"status":"ok","timestamp":1604262295413,"user_tz":300,"elapsed":7773,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"39c63400-0da9-4690-de87-6215a00fd6bc","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Transform the data back to original units\n","transformed_X_train = scaler.inverse_transform(X_train_scaled)\n","transformed_X_val = scaler.inverse_transform(X_val_scaled)\n","\n","# Evaluate the average of each data set\n","print(f'The mean of the scaled feature in column 0 is {X_train_scaled.iloc[:, 0].mean():.3f} and {X_val_scaled.iloc[:, 0].mean():.3f} for training and validation, respectively.')\n","print(f'The mean of the un-scaled feature in column 0 is {transformed_X_train[:, 0].mean():.3f} and {transformed_X_val[:,0].mean():.3f} for training and validation, respectively.')"],"execution_count":20,"outputs":[{"output_type":"stream","text":["The mean of the scaled feature in column 0 is 0.000 and 0.033 for training and validation, respectively.\n","The mean of the un-scaled feature in column 0 is 0.801 and 0.814 for training and validation, respectively.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wskfl5J1X6IV"},"source":["### Exercises\n","\n","> 1. You should see that the mean of the first feature in `X_train_scaled` is 0, but its mean in `X_val_scaled` is  0.033. Why isn't the mean of `X_val_scaled` 0 after we've standardized it?\n","\n",">> Standardization is based on the mean and standard deviation of the training set. If the validation set has a different mean or standard deviation then it won't have mean 0 and standard deviation 1 (although it should be close!)."]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"CFXNd_jBIufm"},"source":["\n","# Feature Engineering\n","\n","In this section, we will see that our dataset has many features that are not useful for predicting the target. When this is the case, you should try to collect better data, however, when better data in unavailable we must do the best with what we have. One option is to create new, strong features that are more useful for predicting the target by transforming and combining weak features (like we did in Lab 1!). This practice is known as feature engineering.\n","\n","Generally, good features have several desirable characteristics, for example, they:\n","*   are spread out evenly (e.g., Gaussian or uniform distributions)\n","*   behave differently (e.g., they may have different means) when the target is different\n","*   are normalized to similar scales.\n","\n","One common technique is to apply logarithms or square roots to features with very large values that follow a distribution with a long tail (e.g., population, income). The problem with distributions that have large values and long tails is that the absolute change in the value of a feature gives less information than the relative change. For example, a 100 dollar difference in income means very different things if you are comparing two low income data points versus two high income data points.\n","\n","We can also \"combine\" features (e.g., add them, multiply them) to make them more useful. When we combine features we can take advantage of *interaction effects* (a concept from statistics), where the interaction of two features together is greater than the sum of the parts. For example, if two features correlate strongly with a target, it is likely that the product of the two features will also correlate. \n","\n","Constructing good features requires experience, domain expertise, and some luck. If you have domain knowledge about your application then you should  think about whether a transformation or a new feature makes sense for the problem, and you should always check that your engineered features are useful for predicting a target. You can generally do anything to create new features. The common tricks include multiplying and/or dividing multiple existing features, however, there are two major rules related to feature engineering:\n","\n","1. Do not create too many features, especially in order to chase marginal increases in correlation. If the correlation is small and your dataset is small, you might be creating fake (dataset-dependent) correlations that wont generalize outside of your data.\n","2. You **cannot** use the target variable to create a feature."]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"Euay2rOoIufm"},"source":["## Transforming your data to get better features\n","\n","In this lab, we will only do feature engineering by exploiting interaction effects between multiple features. The best way to go about this is by domain knowledge and developed intuition, however, there are also some tricks to make features without domain knowledge, which will be the focus of this section. \n","\n","An algorithmic way to generate features is provided in the code block below. Where we loop through every pair of features, and multiply them features together (i.e., \"combine\" them). We then evaluate the _usefulness_ of the combined features. There are many metrics for feature selection ([see list of `sklearn` functions here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection)). We will use the function `f_classif` to evaluate the  **An**alysis **o**f **Va**riance (ANOVA) F-Value, which is appropriate for estimating how useful a feature is for predicting a categorical target. A higher F-value suggests the feature will be better, and a $p$-value $<$ 0.05 suggests the F-value returned is based on a real trends in the data (i.e., not random chance). \n","\n","For more detail on the ANOVA F-Value you can refer to[this article](https://machinelearningmastery.com/feature-selection-with-numerical-input-data/#:~:text=ANOVA%20f%2Dtest%20Feature%20Selection,-ANOVA%20is%20an&text=The%20ANOVA%20method%20is%20a,as%20an%20ANOVA%20f%2Dtest.&text=The%20results%20of%20this%20test,be%20removed%20from%20the%20dataset). However, please keep in mind that `f_classif` should only be used when the target is categorical. As a result, you should use other tests if the target is continuous. \n","\n","**Note:** The code block below uses a 'magic command' ([more info here](https://ipython.readthedocs.io/en/stable/interactive/magics.html?highlight=capture#cellmagic-capture )) to suppress warning messages that would normally come up when running `f_classif` in this example. Those warnings can be safely ignored, and this magic command provides and easy way to hide them :)  \n"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"exOOv6dNIufn","executionInfo":{"status":"ok","timestamp":1604262295811,"user_tz":300,"elapsed":8160,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"e07ea430-36ad-4d46-f3b7-cc104cbb7d84","colab":{"base_uri":"https://localhost:8080/"}},"source":["%%capture --no-stdout  \n","# from sklearn.feature_selection import f_classif\n","# Create a list of F-values for the existing features\n","feature_F_scores, _ = f_classif(X_train, y_train)\n","\n","# Iterate through each combination of features\n","for f1_index, f1 in enumerate(X_train.columns):\n","  for f2_index, f2 in enumerate(X_train.columns[f1_index + 1:]):\n","    # Multiply the two features to create a new feature\n","    new_feature = X_train[[f1]].multiply(X_train[f2], axis=0)\n","    # Evaluate F-value of new feature\n","    F_Score_new, p_value_new = f_classif(new_feature, y_train)\n","    # Evaluate the relative improvement of the new feature\n","    F_score_improvement = F_Score_new[0]/max(feature_F_scores[[f1_index, f2_index]])\n","    # Print out features that is sufficiently improved \n","    if F_score_improvement >= 1.5 and F_Score_new[0] >= 75 and p_value_new < 0.05:\n","        '''Note that F_score_improvement >= 1.5 and F_Score_new[0] >= 75 is\n","         relatively arbitrary, and that other values could be used.'''\n","        print(f'{f1} + {f2} has an F-score of {F_Score_new[0]:.2f}')\n","        print(f'\\tBetter by a factor of {F_score_improvement:.2f} over features in isolation')\n","        print(f'\\tThe result is significant (p = {p_value_new})')\n"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Installment + InqLast6mths has an F-score of 134.11\n","\tBetter by a factor of 8.57 over features in isolation\n","\tThe result is significant (p = [1.01107633e-30])\n","DaysWithCrLine + InqLast6mths has an F-score of 96.81\n","\tBetter by a factor of 6.18 over features in isolation\n","\tThe result is significant (p = [1.08827276e-22])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"RyYDpyD-Iufp"},"source":["### Exercise\n","\n","> 1. Play around with the above code and the space below to see if you can find any new interesting or useful features. For example, you could try different transforms of the features, or combining three features together. Oftentimes, this task of feature engineering is the most important and time consuming aspect of machine learning. Good features are hard to come by!\n","\n",">> You can try changing the function we use to combine features, adjusting the various threshold defined in the final id statement\n","\n","## Using transformed features to train a model\n","\n","It is important to not get too carried away with algorithmic feature engineering. If we construct a new feature with domain knowledge, we should think carefully about why it is relevant. If you have a small amount of data, you don't want to create idiosyncrasies in your dataset that don't reflect real life. The danger with feature engineering, is that you may create features that guide your model towards patterns that don't actually exist (i.e., overfitting).\n","\n","### Exercise\n","> 1. In the code below, we've proposed a few additional useful features and created a function that adds them to the dataset (based on the previous code block). Feel free to add any additional features, or change the existing ones, and try to improve the precision and recall."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"fbInOhsdIufq","executionInfo":{"status":"ok","timestamp":1604262300628,"user_tz":300,"elapsed":12969,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"1d48ec68-a99a-4130-e7e9-40fabf883333","colab":{"base_uri":"https://localhost:8080/"}},"source":["technique_name = 'Feature Engineering'\n","\n","def new_feature_combos(X):\n","   \n","    # First, we apply the monotone transform function\n","    X_new = X.copy()\n","    \n","    # Then we add new features\n","    X_new['Installment * InqLast6mths'] = X_new.Installment * X_new.InqLast6mths\n","    X_new['DaysWithCrLine * InqLast6mths'] = X_new.DaysWithCrLine * X_new.InqLast6mths\n","\n","    # Add any additional features that you might have found here\n","    # ----------------------------------------------------------\n","    \n","    \n","    \n","    # ----------------------------------------------------------\n","    return X_new\n","\n","# Make new X features with interactions\n","X_train_interactions = new_feature_combos(X_train)\n","X_val_interactions = new_feature_combos(X_val)\n","\n","# Fit and score the model, save it to the all models dictionary\n","all_models = fit_and_score_model(all_models, technique_name, X_train_interactions, X_val_interactions, y_train, y_val)\n","compare_models(technique_name)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"stream","text":["LR_L2 achieved a precision of 0.643 and recall of 0.019\n","LR_L1 achieved a precision of 0.259 and recall of 0.591\n","CART achieved a precision of 0.190 and recall of 0.174\n","RF achieved a precision of 0.600 and recall of 0.006\n","On average, scores improved by 0.13, and the most improvement was 0.33\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wSUooLYFp0IC"},"source":["# Feature selection\n","\n","Often times we have a lot of features that we don't actually need and keeping those extra features (i.e., features that are not useful for predicting a target) can reduce model performance. We can use feature section to pick the best set of features for our model and address this issue."]},{"cell_type":"markdown","metadata":{"id":"4NQ_wA00pJAv"},"source":["## Doing feature selection\n","\n","As in the previous section, we will use F-values to evaluate how _useful_ a feature is for predicting a target. We will also introduce a new function `SelectKBest` from `sklearn` that picks the `k` best features, where \"best\" is quantified by a function (e.g., `f_classif`)."]},{"cell_type":"code","metadata":{"id":"Gmg4igdGp2kV","executionInfo":{"status":"ok","timestamp":1604262303178,"user_tz":300,"elapsed":15510,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"ece91eea-f3d0-422d-be5d-77433f992bd4","colab":{"base_uri":"https://localhost:8080/"}},"source":["technique_name = 'Feature_Selection'\n","\n","# Standardize the new features\n","select_features = SelectKBest(f_classif, k=10)\n","select_features = select_features.fit(X_train, y_train)\n","\n","# Get mask of columns that have good features\n","feature_mask = select_features.get_support()\n","X_train_feature_selection = X_train.iloc[:, feature_mask]\n","X_val_feature_selection = X_val.iloc[:, feature_mask]\n","\n","# Fit and score the model, save it to the all models dictionary\n","all_models = fit_and_score_model(all_models, technique_name, X_train_feature_selection, X_val_feature_selection, y_train, y_val)\n","compare_models(technique_name)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["LR_L2 achieved a precision of 0.733 and recall of 0.023\n","LR_L1 achieved a precision of 0.259 and recall of 0.572\n","CART achieved a precision of 0.233 and recall of 0.226\n","RF achieved a precision of 0.333 and recall of 0.015\n","On average, scores improved by 0.12, and the most improvement was 0.38\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oRr6lEY0uyNE"},"source":["### Exercises\n","\n","> 1. What's the fewest number of features you can use to get a positive average improvement over the baseline functions?\n"]},{"cell_type":"code","metadata":{"id":"VwdftBqsvAF6","executionInfo":{"status":"ok","timestamp":1604262303491,"user_tz":300,"elapsed":15813,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"d0aa897c-0955-42ea-dd57-2bd7a761bd45","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Write your code here\n","# ----------------------------------------------------------\n","\n","select_features = SelectKBest(f_classif, k=1)\n","select_features = select_features.fit(X_train, y_train)\n","\n","# Get mask of columns that have good features\n","feature_mask = select_features.get_support()\n","X_train_feature_selection = X_train.iloc[:, feature_mask]\n","X_val_feature_selection = X_val.iloc[:, feature_mask]\n","\n","# Fit and score the model, save it to the all models dictionary\n","all_models = fit_and_score_model(all_models, technique_name, X_train_feature_selection, X_val_feature_selection, y_train, y_val)\n","compare_models(technique_name)\n","\n","# ----------------------------------------------------------\n"],"execution_count":24,"outputs":[{"output_type":"stream","text":["LR_L2 achieved a precision of 0.000 and recall of 0.000\n","LR_L1 achieved a precision of 0.279 and recall of 0.312\n","CART achieved a precision of 0.279 and recall of 0.312\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"stream","text":["RF achieved a precision of 0.279 and recall of 0.312\n","On average, scores improved by 0.05, and the most improvement was 0.21\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-OAKJTGFvAdH"},"source":["> 2. Based on this result, what do you think is/are the most important feature(s)?"]},{"cell_type":"code","metadata":{"id":"4VeL_YfMuyXu","executionInfo":{"status":"ok","timestamp":1604262303767,"user_tz":300,"elapsed":16080,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"2408a6be-b49d-459d-c99d-f86d91a25284","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Write your code here\n","# ----------------------------------------------------------\n","\n","X_train.columns[feature_mask]\n","\n","# ----------------------------------------------------------"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['CreditPolicy'], dtype='object')"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"vjgS2d6pORdu"},"source":["# Grid search\n","\n","Choosing hyperparameters for a machine learning model is non-trivial, and many of the machine learning models that we use have a wide variety of hyperparameters. For example, logistic regression on scikit-learn allows 'L1' or 'L2' regularization, a regularization weight 'C', and additional modifications to the loss function (see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for full list). Decision trees have parameters for depth, loss function, and splitting rules (see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) for full list). \n","\n","When we are tuning a single parameter, the common method is to use k-fold cross-validation and evaluate for which setting does the model perform the best. However, how do we tune 5 or 6 different parameters all at once? The standard method is known as _grid-searching_, where we simply do k-fold cross-validation on all possible combinations of the different parameter values. In grid searching, the practice is to define a specific scoring function, perform a search, and identify an appropriate model. Of course, this approach is computationally expensive, especially when there are a large number of parameters or values to consider. Interestingly, researchers have found that randomly searching the parameter space is often just as effective as grid searching. What this suggests is that there is often no good intuitive way to choose hyperparameters.\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"JoE9YUcoORdu"},"source":["## Parameter tuning via grid search\n","\n","Scikit-learn provides two useful functions `GridSearchCV()` and `RandomSearchCV()` to implement two approaches towards selecting the hyperparameters. These functions take in a  model, a dictionary of parameters over which to sweep, and a scoring function to evaluate all of the different models on. For each parameter combination, we will run the function with 5-fold cross validation to evaluate each of the scoring functions. Scikit-learn provides a [number of predefined scoring functions](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter), but you can also make your own.\n","\n","In this section we will do a grid search to find better hyperparameters of the `DecisionTreeClassifier()`. We will search over four sets of hyperparameters listed below with each combination of arguments listed in the brackets:\n","* `criterion` ('gini' or 'entropy'): sets the loss function on which to make splits.\n","* `min_samples_leaf` (1, 2, 5, 10, or 20): the minimum number of samples for a node to be a leaf\n","* `max_features` ('auto', 'log2', None):  the maximum number of features to consider when searching for splits \n","* `class_weight` ('balanced', None): balances the penalty for misclassified 1's versus 0's"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"5xykbyn2ORdv","executionInfo":{"status":"ok","timestamp":1604262316624,"user_tz":300,"elapsed":28926,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"ca805a7a-5537-473a-9c59-3c82697023c1","colab":{"base_uri":"https://localhost:8080/"}},"source":["\"\"\"Remember this is training a CART tree for every combination of parameters\n","in the list params_to_search... this might take 10 to 20 seconds to run :)\"\"\"\n","\n","# Dictionary of parameters to search\n","params_to_search = {\n","    'criterion': ['gini', 'entropy'],\n","    'min_samples_leaf': [1, 2, 5, 10, 20],\n","    'max_features': ['auto', 'log2', None],\n","    'class_weight': ['balanced', None],\n","}\n","\n","# Initialize a model\n","mdl = DecisionTreeClassifier(random_state=0)\n","# Initialize the grid search\n","optimized_dt = GridSearchCV(mdl, params_to_search, scoring = ['recall', 'precision'], refit=False, cv=5)\n","# Run the grid search\n","optimized_dt.fit(X_train, y_train)\n"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv=5, error_score=nan,\n","             estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n","                                              criterion='gini', max_depth=None,\n","                                              max_features=None,\n","                                              max_leaf_nodes=None,\n","                                              min_impurity_decrease=0.0,\n","                                              min_impurity_split=None,\n","                                              min_samples_leaf=1,\n","                                              min_samples_split=2,\n","                                              min_weight_fraction_leaf=0.0,\n","                                              presort='deprecated',\n","                                              random_state=0, splitter='best'),\n","             iid='deprecated', n_jobs=None,\n","             param_grid={'class_weight': ['balanced', None],\n","                         'criterion': ['gini', 'entropy'],\n","                         'max_features': ['auto', 'log2', None],\n","                         'min_samples_leaf': [1, 2, 5, 10, 20]},\n","             pre_dispatch='2*n_jobs', refit=False, return_train_score=False,\n","             scoring=['recall', 'precision'], verbose=0)"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"fYmC9I3QORdx"},"source":["The results from grid searching are all stored in `optimized_dt.cv_results_`.\n","\n","### Exercise\n","> 1. Take a look at the keys and values of `optimized_dt.cv_results_`, and make a scatter plot of the `mean_test_precision` versus `mean_test_recall`."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"qsIQsKjBORdx","executionInfo":{"status":"ok","timestamp":1604262316812,"user_tz":300,"elapsed":29105,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"d2636a24-63ce-4bed-80d9-ad23191a802e","colab":{"base_uri":"https://localhost:8080/","height":279}},"source":["# Get the mean test precision and precision from optimized_dt.cv_results_\n","# and plot a scatter plot.\n","\n","# Write your code here\n","# ---------------------------------------------------------\n","\n","# Get the average precision from the validation sets in cross validation\n","mean_test_precision = optimized_dt.cv_results_['mean_test_precision']\n","mean_test_recall = optimized_dt.cv_results_['mean_test_recall']\n","\n","# Plot precision and recall as a scatter plot\n","plt.scatter(mean_test_precision, mean_test_recall)\n","plt.xlabel('Precision');\n","plt.ylabel('Recall');\n","\n","# ---------------------------------------------------------"],"execution_count":27,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"kiVSFzbnORdz"},"source":["After the grid search you should see that the CART tree performs nearly as well as the best LR_L1 model. However, it seems that if you select the parameters appropriately, you will either swing to high recall or high precision.\n","\n","You can go through all of the parameters (`params`) and their corresponding precision (`mean_test_precision`) and recall scores (`mean_test_recall`) inside `cv_results_`. Explore the grid search results and answer the following questions.\n","\n","\n","1. Lets suppose that the \"best\" model is one that maximizes the sum of mean test recall and mean test precision. What parameters does the \"best\" model have based on the results from the grid search?\n"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"HP3Wd8xeORdz","executionInfo":{"status":"ok","timestamp":1604262316813,"user_tz":300,"elapsed":29094,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"ed8d2217-0c10-4cbc-c3e6-f1b107495208","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Write your code here\n","# -------------------\n","\n","cv_result_df = pd.DataFrame(optimized_dt.cv_results_)\n","cv_result_df['mean_test_score'] = cv_result_df['mean_test_recall'] + cv_result_df['mean_test_precision']\n","highest_mean_score_index = cv_result_df.mean_test_score.argmax()\n","\n","print(f'The best model has a score of {cv_result_df.mean_test_score.max():.3f}')\n","print(f'The best model has the parameters {cv_result_df.params.loc[highest_mean_score_index]}')\n","\n","# -------------------"],"execution_count":28,"outputs":[{"output_type":"stream","text":["The best model has a score of 0.742\n","The best model has the parameters {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'auto', 'min_samples_leaf': 20}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5q5cgvEmHqA0"},"source":["2. Determine the set of parameters that achieved the highest precision and the other set that achieved the highest recall. Do the settings make sense intuitively? "]},{"cell_type":"code","metadata":{"id":"xsi3RoQTHqM0","executionInfo":{"status":"ok","timestamp":1604262316816,"user_tz":300,"elapsed":29087,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"372d5765-b604-4a92-a259-a63cf8fa219a","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Write your code here\n","# -------------------\n","\n","'''You can write some code to explore the data below\n","You should find that setting balanced class weights will help recall.\n","In general, increasing the minimum samples per leaf improves performance,\n","and setting no class weights will improve precision. This makes sense since\n","the dataset is imbalanced and it is often easier to just try to get a few 1's correctly\n","in order to improve the precision.'''\n","\n","highest_precision = cv_result_df.params[mean_test_precision.argmax()]\n","highest_recall = cv_result_df.params[mean_test_recall.argmax()]\n","print(f'The model with the highest precision had the settings: {highest_precision}\\n \\\n","The model with the highest recall had the settings: {highest_recall}')\n","\n","# -------------------"],"execution_count":29,"outputs":[{"output_type":"stream","text":["The model with the highest precision had the settings: {'class_weight': None, 'criterion': 'gini', 'max_features': 'auto', 'min_samples_leaf': 20}\n"," The model with the highest recall had the settings: {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'auto', 'min_samples_leaf': 20}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"yYsFA93tORd3"},"source":["In the code below, we choose the parameter settings that maximize the sum of precision and recall, and tested the models on the validation set."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"7NwhwC1NORd3","executionInfo":{"status":"ok","timestamp":1604262316992,"user_tz":300,"elapsed":29255,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"b5c1fe4d-b89d-44a8-a331-6ab6ad918f4a","colab":{"base_uri":"https://localhost:8080/"}},"source":[" # Find the model with the highest sum of precision and recall\n","params = optimized_dt.cv_results_['params']\n","best_model_params_index = (mean_test_precision + mean_test_recall).argmax()\n","best_model_params = params[best_model_params_index]\n","\n","# Initialize the model \n","best_model = DecisionTreeClassifier(random_state=0, **best_model_params)\n","'''Note, the ** from above allows us to use a dictionary to set \n","the parameters of a function'''\n","\n","# Fit the model on the ENTIRE training set \n","best_model.fit(X_train, y_train)\n","\n","# Evaluate model precision, recall, and score\n","model_precision = precision_score(y_val, best_model.predict(X_val))  # evaluate precision on test set\n","model_recall = recall_score(y_val, best_model.predict(X_val))  # evaluate recall on test set\n","model_score = (model_precision + model_recall) / 2\n","\n","# Add model scores to all_models data frame\n","all_models.loc['CART', 'Grid Search'] = (model_precision, model_recall, model_score, best_model)\n","all_models['Score'].loc['CART', ['Baseline', 'Grid Search']]"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["model names  technique  \n","CART         Baseline       0.201266\n","             Grid Search    0.349308\n","Name: Score, dtype: float64"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"eeZYe7zEN1Bc"},"source":["### Exercises\n","> 1. Why does precision and recall get worse after training the model on the training set?\n","\n",">> There is a very slight degree of overfitting in the grid search. Remember that in grid search we do cross validation of different splits on the training data. It is only natural for the model to perform a little bit worse on completely out of sample data.\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"IzBIPuaVIufr"},"source":["# Clustering\n","The goal of clustering is to bin observations from a dataset into groups of similar points. Unlike regression, clustering is a method that does not require samples of the target data in order to train the model. This is known as _unsupervised learning_. Clustering is often used early on in model engineering to create new features. In this lab, you will cluster data and develop regression models that use the cluster labels as a feature."]},{"cell_type":"markdown","metadata":{"id":"wgh-5cC-XOpa"},"source":["\n","## Distance metrics\n","Clusters are formed from points separated by small distances. Note that there are two types of distance in cluster (1) _Intra_-cluster distance is the distance between points within a cluster, and (2) _inter_-cluster distance is the distance between different clusters. Intra-cluster distance is the metric that $k$-means clustering minimizes, however, there are other methods that use both types of distance (e.g., [agglomerative clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)). The distance is quantified by a chosen _distance measure_ (see Appendix A for a summary of common distance measures).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SKPPeBfcXPNJ"},"source":["## $k$-Means clustering\n","\n","In this lab, we focus on $k$-means clustering, which is a method for assigning each observation in a given dataset to one of $k$ clusters. For example, you may have a data set of $n$ observations that are indexed by $i = 1, 2, 3... n$ with $m$ features that are indexed by $j = 1, 2, 3... m$. Then we can represent each observation as a point in $m$-D space: $\\mathbf{x}_i = (x_{i1},x_{i2},...,x_{ij})$. \n","\n","$k$-means clustering is a quick and dirty approach to unsupervised learning. It is relatively easy to implement and is generally more efficient than other clustering methods, which makes it useful for large datasets. The main parameter to consider is the number of clusters $k$. The algorithm then finds $k$ optimally-placed cluster centroids. The nearest centroid for a given observation determines the cluster that the observation is binned in.\n","\n","\n","Finding the optimal cluster centroids is an _NP-hard_ problem, and iterative heuristics are consequently used. The most common iterative heuristics is _Lloyds algorithm_, which completes the following steps in each iteration:\n","\n","1. Assign each point to a cluster, based on the nearest centroid.\n","2. Shift each cluster centroid to the centroid of the points in the given cluster.\n","\n","Lloyds algorithm relies on an initial set of centroids (initialized by a random number generator). In practice, randomly generated points that are far apart make for a good set of initial centroids. The algorithm should be run several times to find a good set of centroids. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"cc643Vks0evw"},"source":["## Clustering data\n","Determining the number of clusters to generate often requires exploratory analysis or expert opinion. In simple cases the number of clusters can be intuited once the data is visualized. In general, you should try and play with different numbers of cluster numbers. Use the `KMeans` function to divide the data into 10 separate clusters. Note that you must use the `X_train` for this task."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"R78SibL1Iufs","executionInfo":{"status":"ok","timestamp":1604262316994,"user_tz":300,"elapsed":29254,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}}},"source":["# Define clustering model \n","mdk_k_means = KMeans(n_init = 1,  # number of different centroid seed initializations (number of times algorithm is run)\n","                   n_clusters=10,  # number of clusters (k)\n","                   random_state = 0)  # random seed for k-means algorithm\n","\n","# Fit the model to the training set\n","mdk_k_means.fit(X_train)\n","\n","# Get cluster assignments for each data point\n","clK = mdk_k_means.labels_\n","\n","# Get the centroid of each cluster\n","Centroids = mdk_k_means.cluster_centers_"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"DSvkHOwRIuft"},"source":["$k$-means clustering tries to identify the optimal placement of the $k$ cluster centroids and bins the data accordingly. The final centroids are saved in the rows of `Centroids`. Complete the following activities:"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"AOS2LP_wIufu"},"source":["### Exercise \n","1. Identify the cluster that stores the most \"educational\" loans. Next, change the random state of mdl_k_means to 2. How does changing the random_state effect the cluster with most \"educational\" loans? Why does this happen?"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"hauhGfSVIufu","executionInfo":{"status":"ok","timestamp":1604262316994,"user_tz":300,"elapsed":29244,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"152ba9e2-73b0-4e9d-8f57-0c48ae01f416","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Write your code here\n","\n","# -------------------\n","\n","## Method 1 \n","cluster_set = set(clK)\n","most_education = 0\n","for c in cluster_set:\n","    education_sum = X_train.Purpose_educational[c==clK].sum()\n","    if education_sum > most_education:\n","        most_education = education_sum\n","        education_cluster = c\n","\n","# Method 2 \n","# Make a series of cluster predictions\n","train_clusters = pd.Series(clK, name = 'cluster', index=X_train.index)\n","\n","# Get cluster with most holidays\n","cluster_df = X_train.join(train_clusters).groupby('cluster').sum()\n","education_cluster = cluster_df.Purpose_educational.idxmax()\n","most_education = cluster_df.Purpose_educational.max()\n","\n","'''The number of education loans in the cluster with the most education loans\n"," changes because of random chance'''\n","\n","print('At most {:.3f} education was assigned to cluster {:.0f}.'.format(most_education,education_cluster))\n","\n","# -------------------"],"execution_count":32,"outputs":[{"output_type":"stream","text":["At most 171.000 education was assigned to cluster 0.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"esNVNVNdIufw"},"source":["2. Change the n_init to 50 to run the algorithm 50 times. Compare the `inertia_` attribute of this new model to that of the first clustering model `mdk_k_means`. Why do you think it improved? (**Hint**: Inertia is the sum of squared intra-cluster distances). \n","\n"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"DrVcdw6sIufw","executionInfo":{"status":"ok","timestamp":1604262319891,"user_tz":300,"elapsed":32133,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"3cc07756-7687-40ae-8491-a024266553b6","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Write your code here\n","\n","# -------------------\n","\n","previous_inertia = mdk_k_means.inertia_\n","\n","# Define clustering model \n","mdk_k_means_new = KMeans(n_init = 50, n_clusters=10, random_state = 0)  # random seed for k-means algorithm\n","mdk_k_means_new.fit(X_train)\n","new_inertia = mdk_k_means_new.inertia_\n","\n","'''The model improved because we're taking the best one from the 50 that \n","are run (rather than just the best one of one).'''\n","\n","print(f'The inertia improved by {(previous_inertia/new_inertia - 1) * 100:.2f}%')\n","\n","# -------------------"],"execution_count":33,"outputs":[{"output_type":"stream","text":["The inertia improved by 5.46%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CIeXawIQBXro"},"source":["In the examples above we arbitrarily set the number of clusters $k$ to 10, however, there are tricks to choosing the right $k$. One common heurstic for choosing the number of clusters is the _elbow method_ (details [here](https://predictivehacks.com/k-means-elbow-method-code-for-python/)), but it's a little contriversial. However, since we're planning to use the clusters as a feature in a model (i.e., stacking, which is discussed in the next section), the best thing to do is to see what number of clusters achieves the best score in the resulting stacked model.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"MYIrgcvVIuf3"},"source":["# Ensembles\n","Ensemble methods combine the knowledge from multiple models to achieve better performance than any of the constituent models could achieve alone. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"rUrgtRowIrQC"},"source":["\n","## Stacking multiple models\n","\n","Stacking is a type of ensemble model that passes the predictions from one or more models as input to another prediction model. In a stacked model, we train a series of *different* prediction models on the dataset. Each of those simple models will predict a target that is then used as a feature(s) for another downstream model. \n","\n","In general, the initial models should do most of the work learning from the data. However, the final model will be able to use those results, and learn when one predictive model is more appropriate than another. `sklearn` actually has a function `StackingClassifier` for stacking models (documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html)). However, we provide an example of stacking that doesn't use `StackingClassifier` so that we can one-hot-encode the cluster numbers.  \n","\n","Let's create a set of four stacked models that end with our four baseline models and start with a clustering model. "]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"ZrEwRU_fIuf3","executionInfo":{"status":"ok","timestamp":1604262324876,"user_tz":300,"elapsed":37109,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"533fa306-028f-463a-e638-5d96e55cf9de","colab":{"base_uri":"https://localhost:8080/"}},"source":["'''A stacked model that begins with clustering'''\n","technique_name = 'Stacking'\n","\n","# Initialize the clustering model\n","model = KMeans(n_init=10, n_clusters=8, random_state=0)\n","model.fit(X_train)\n","\n","# Stack training data\n","X_train_prediction = pd.Series(model.predict(X_train), \n","                            name='cluster',\n","                            index=X_train.index)\n","\n","# One-hot-encode cluster numbers\n","X_train_prediction = pd.get_dummies(X_train_prediction)\n","# Add cluster numbers to features\n","X_train_stacked = X_train.join(X_train_prediction)\n","\n","# Stack testing data\n","X_val_prediction = pd.Series(model.predict(X_val),\n","                             name='cluster',\n","                             index=X_val.index)\n","# One-hot-encode cluster numbers\n","X_val_prediction = pd.get_dummies(X_val_prediction)\n","# Get cluster numbers that weren't predicted in the testing set\n","missing_cluster_columns = X_train_prediction.columns.difference(X_val_prediction.columns)\n","# Add missing cluster dummy variables\n","X_val_prediction[missing_cluster_columns] = 0 \n","# Add cluster numbers to features\n","X_val_stacked = X_val.join(X_val_prediction)\n","\n","# We will use logistic regression instead of a decision tree at the higher-level classifier\n","all_models = fit_and_score_model(all_models, technique_name, X_train_stacked, X_val_stacked, y_train, y_val)\n","compare_models(technique_name)"],"execution_count":34,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"stream","text":["LR_L2 achieved a precision of 0.000 and recall of 0.000\n","LR_L1 achieved a precision of 0.260 and recall of 0.589\n","CART achieved a precision of 0.195 and recall of 0.199\n","RF achieved a precision of 1.000 and recall of 0.004\n","On average, scores improved by 0.11, and the most improvement was 0.42\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8-9hipLhQF9z"},"source":["### Exercise \n","\n","> 1. Change the number of clusters to maximize the biggest improvement between these models and the baseline models. How many clusters do you think we should use in this stacking model?\n","\n",">> 8 seems to be the best"]},{"cell_type":"markdown","metadata":{"id":"sUvSnFvtmELt"},"source":["## Bagging Models\n","Bagging is another type of ensemble model that passes the prediction from one or more models as input to a _voting rule_. For example, random forests are an bagging model that is constructed from a family of simple decision trees. Random forests (and ensemble methods in general) work on the assumption that most decision trees will predict well, and they define a simple voting rule (e.g., majority rule) to convert the predictions from several models into a single prediction. Similar to stacking, `sklearn` has a function `VotingClassifer` for bagging models (documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html))\n","\n","Throughout this lab we have found that some models clearly performed better than others on both precision and recall. However, even if the average score of one model is less than another, that model may still be better at predicting for certain subsets and scenarios. This is where the idea of ensembles comes in. Below, we use the `VotingClassifer` to fit a bagging model. Note, that we 'weight' the predictions of each model using the model's score, to give better models more sway (i.e., votes) than worse models. "]},{"cell_type":"code","metadata":{"id":"gAJ6W7KQmEjB","executionInfo":{"status":"ok","timestamp":1604262328525,"user_tz":300,"elapsed":40751,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"05567c8b-415d-4cec-c5f6-c86232964bb0","colab":{"base_uri":"https://localhost:8080/"}},"source":["# from sklearn.ensemble import VotingClassifier\n","technique_name = 'Bagging'\n","\n","# Get a list of initialized models for bagging\n","models_dict = make_models()\n","# Convert models to list of tuples (required by documentation)\n","model_list = list(models_dict.items()) \n","\n","# Set the weights for each model as their relative performance\n","bagging_weights = all_models.Score[:,'Baseline'].values\n","\n","# Initialize bagging model\n","bagging_model = VotingClassifier(model_list, weights=bagging_weights)\n","# Fit the bagging model (i.e., each of the four models that are bagged)\n","bagging_model.fit(X_train.values, y_train.values)\n","\n","# Predict the target using the bagged model\n","bagging_prediction = bagging_model.predict(X_val)\n","\n","# Evaluate ensemble model\n","model_precision = precision_score(y_val,  bagging_prediction)  # evaluate precision on validation set\n","model_recall = recall_score(y_val,  bagging_prediction)  # evaluate recall on validation set\n","model_score = (model_precision + model_recall) / 2\n","\n","# Add ensemble model to all_models dataframe\n","all_models.loc[:, technique_name, :] = (model_precision, model_recall, model_score, None) \n","compare_models(technique_name)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["On average, scores improved by 0.24, and the most improvement was 0.42\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bOVA9RmfY15B"},"source":["### Exercises\n","\n","> 1. You should see that this model was no better than the best model from the set of baselines. Why do you think that bagging all of these models was no better than the best of the constituent models?\n","\n",">> We bagged several low quality models\n","\n","> 2. Code up the bagging model above without using the `VotingClassifier`. You predictions should be identical to what we see above. "]},{"cell_type":"code","metadata":{"id":"agoFUpKZV1ij","executionInfo":{"status":"ok","timestamp":1604262328525,"user_tz":300,"elapsed":40741,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"a44ec0d7-c569-4b46-d53a-dafe782897b7","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Write your code here\n","\n","# -------------------\n","\n","# Define list of models in ensemble\n","ensemble_models_df = all_models.loc[:, ['Baseline'], :]\n","ensemble_models = ensemble_models_df.Model\n","\n","# Make the weights for the voting rule\n","ensemble_weighting = ensemble_models_df.Score / ensemble_models_df.Score.sum()\n","\n","# Initialize a numpy array of zeros of hold predictions\n","y_preds = np.zeros((len(y_val), len(ensemble_models)))\n","\n","# Loop through each model and make prediction of the validation set\n","for model_idx, model in enumerate(ensemble_models):\n","  y_preds[:, model_idx] = model.predict(X_val) * ensemble_weighting[model_idx]\n","\n","# Take the highest prediction to be the prediction of the ensemble \n","ensemble_prediction = np.round(y_preds.sum(axis=1))\n","\n","# Evaluate ensemble model\n","model_precision = precision_score(y_val,  ensemble_prediction)  # evaluate precision on validation set\n","model_recall = recall_score(y_val,  ensemble_prediction)  # evaluate recall on validation set\n","model_score = (model_precision + model_recall) / 2\n","\n","# Add ensemble model to all_models data frame\n","all_models.loc[:, 'Bagging', :] = (model_precision, model_recall, model_score, None) \n","compare_models('Bagging')\n","\n","# -------------------\n"],"execution_count":36,"outputs":[{"output_type":"stream","text":["On average, scores improved by 0.24, and the most improvement was 0.42\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L3RWe_sel8Rf"},"source":["# Analyzing the performance of all models\n","\n","Lets finish up by looking at the `all_models` data frame. \n","\n","### Exercise\n","\n","> 1. What model achieved the highest recall, the highest precision, and the highest score?  \n","\n"]},{"cell_type":"code","metadata":{"id":"LXeIrA6aVNGf","executionInfo":{"status":"ok","timestamp":1604262328901,"user_tz":300,"elapsed":41106,"user":{"displayName":"Craig Fer","photoUrl":"","userId":"10199331155832657902"}},"outputId":"7be2b164-a9d5-44df-a179-378fa88a92d1","colab":{"base_uri":"https://localhost:8080/","height":166}},"source":["# Write your code here\n","# -------------------\n","\n","# Get the models with the best precision, recall, and score\n","best_models = all_models[['Precision',\t'Recall',\t'Score']].idxmax()\n","# Return the values \n","all_models.loc[best_models]\n","\n","# -------------------"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Score</th>\n","      <th>Model</th>\n","    </tr>\n","    <tr>\n","      <th>model names</th>\n","      <th>technique</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>RF</th>\n","      <th>Stacking</th>\n","      <td>1.000000</td>\n","      <td>0.004193</td>\n","      <td>0.502096</td>\n","      <td>(DecisionTreeClassifier(ccp_alpha=0.0, class_w...</td>\n","    </tr>\n","    <tr>\n","      <th>LR_L1</th>\n","      <th>Feature Engineering</th>\n","      <td>0.259191</td>\n","      <td>0.591195</td>\n","      <td>0.425193</td>\n","      <td>LogisticRegression(C=1.0, class_weight='balanc...</td>\n","    </tr>\n","    <tr>\n","      <th>RF</th>\n","      <th>Stacking</th>\n","      <td>1.000000</td>\n","      <td>0.004193</td>\n","      <td>0.502096</td>\n","      <td>(DecisionTreeClassifier(ccp_alpha=0.0, class_w...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 Precision  ...                                              Model\n","model names technique                       ...                                                   \n","RF          Stacking              1.000000  ...  (DecisionTreeClassifier(ccp_alpha=0.0, class_w...\n","LR_L1       Feature Engineering   0.259191  ...  LogisticRegression(C=1.0, class_weight='balanc...\n","RF          Stacking              1.000000  ...  (DecisionTreeClassifier(ccp_alpha=0.0, class_w...\n","\n","[3 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"S8xtFuv2VNPp"},"source":["> 2. Think about your application. Which models are useful? Are there certain models that are definitively better than others? \n","\n",">> The model that achieve high recall (LR_L1 with\tFeature Engineering) is likely the best in this application. The stacked RF model performed a very well on the score because it achieved perfect precision (i.e., no false positives), however, that model only predicted that 2 people failed to repay their loan, which misses 475 other people who failed to repay. "]},{"cell_type":"markdown","metadata":{"id":"t-A6FDAU1h4G"},"source":["# Closing comments\n","In the end, there are very few rules in terms of what you should and shouldn't do when model engineering. Model engineering is more of an art than a science. Keep in mind that a method that works really well for one application may not work at all for another.  You can add new components in the stack, grid search to find better parameters of the top model, hunt for new features, and train different models on different subsets of data (e.g., bagging/bootstrap). It's even possible to train your models on the prediction errors of other models (e.g., boosting). "]},{"cell_type":"markdown","metadata":{"id":"jxtMlKXx5VvN"},"source":["# Appendix A: Distance measures\n","\n","Let $\\mathbf{x} = (x_1,x_2,\\dots,x_n)$ and $\\mathbf{y} = (y_1,y_2,\\dots,y_n)$ be two observations. A distance metric is defined as a function $d(\\mathbf{x},\\mathbf{y})$, that satisfies the following criteria:\n","\n","1. __Non-negativity:__ $d(\\mathbf{x},\\mathbf{y}) \\geq 0$ and $d(\\mathbf{x},\\mathbf{y}) = 0 \\Rightarrow \\mathbf{x} = \\mathbf{y}$.\n","2. __Symmetry:__ $d(\\mathbf{x},\\mathbf{y}) = d(\\mathbf{y},\\mathbf{x})$.\n","3. __Triangle inequality:__ $d(\\mathbf{x},\\mathbf{y}) + d(\\mathbf{y},\\mathbf{z}) \\geq d(\\mathbf{x},\\mathbf{z})$.\n","\n","Any distance metric can be used for clustering. Moreover, metric-like functions that satisfy some of the constraints---often ignoring the triangle inequality---are also used in practice. In this section, we summarize several commonly used distance functions. Custom distance functions can also be used, however, for most purposes those contained in `pdist` are sufficient.\n","\n","1. __Euclidean (i..e, $l_2$ distance):__ the standard notion of distance that we are most familiar with:\n","    $$ d(\\mathbf{x},\\mathbf{y}) = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2}.$$\n","    \n","2. __Mahalanobis:__ intuitively tries to account for distortions in the data by normalizing with the covariance. Let $K_{i,j}$ be the $i^th$ and $j^{th}$ covariance matrix. Then, the Mahalanobis distance measures:\n","    $$d(\\mathbf{x},\\mathbf{y}) = \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2 K_{i,j}}.$$\n","    \n","3. __Manhattan/Cityblock (i.e., $l_1$ distance):__ sum of the absolute values of distances:\n","    $$ d(\\mathbf{x},\\mathbf{y}) = \\sum_{i=1}^n | x_i - y_i |.$$\n","\n","4. __Chebychev (i.e., $l_\\infty$ distance):__ largest distance of a single feature:\n","    $$ d(\\mathbf{x}, \\mathbf{y}) = \\max_{i} | x_i - y_i |.$$\n","\n","5. __Minkowski (i.e., $l_p$ distance):__ a generalization of Euclidean, Manhattan, and Chebyshev:\n","    $$ d(\\mathbf{x},\\mathbf{y}) = \\sqrt[p]{\\sum_{i=1}^n |x_i - y_i|^p}.$$\n","\n","6. __Hamming:__ the number of different features between the two vectors:\n","    $$ d(\\mathbf{x},\\mathbf{y}) = \\sum_{i=1}^n \\mathbb{I}(x_i \\neq y_i).$$\n","    "]}]}