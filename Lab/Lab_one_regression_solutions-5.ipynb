{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_one_regression_solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_1GISKe768k",
        "colab_type": "text"
      },
      "source": [
        "<center>\n",
        "    <h3>University of Toronto</h3>\n",
        "    <h3>Department of Mechanical and Industrial Engineering</h3>\n",
        "    <h3>MIE368 Analytics in Action </h3>\n",
        "    <h3>(Fall 2020)</h3>\n",
        "    <hr>\n",
        "    <h1>Lab 1: Linear and Logistic Regression</h1>\n",
        "    <h3>September 23, 2020</h3>\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soIx6yNt8SBd",
        "colab_type": "text"
      },
      "source": [
        " # Introduction\n",
        "\n",
        "Linear and logistic regression are two of the most widely used methods in analytics. They are used to model the relationship between a _target_ variable (i.e., dependent variable and one or more _features_ (i.e., independent variables). The application of these methods typically falls into one of two categories based on the end goal:\n",
        "\n",
        " 1. __Prediction:__ estimate the target based on given features. The goal is to obtain a model that performs well on out-of-sample data (i.e., data it hasn't seen before).\n",
        "\n",
        " 2. __Information extraction:__ reveal how the target is associated with the feature variables. Care is needed with respect to model assumptions when seeking to extract information.\n",
        "\n",
        "__By the end of this lab you will be able to:__\n",
        "> i) Apply linear regression to predict a continuous target.  \n",
        "> ii) Apply logistic regression to predict a binary target.  \n",
        "> iii) Apply feature selection and regularization to construct better models.  \n",
        "> iv) Evaluate model fitness using $R^2$, mean accuracy, precision-recall, confusion matrices, and ROC curves.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BgKbVT1UNzF",
        "colab_type": "text"
      },
      "source": [
        "# Application\n",
        "\n",
        "In this lab, we will use data from the National Basketball League (NBA). We have two goals: (1) to understand and predict the relationship between stats and total number of wins, and (2) to understand and predict the relationship between stats and whether a team will make the playoffs. \n",
        "Our data includes team data from all NBA seasons between 1980 and 2016 except for the two lockout seasons (1999 and 2012). For a brief introduction to basketball, please watch the first 95 seconds of the following video:\n",
        "\n",
        "<a href=\"https://www.youtube.com/watch?v=wYjp2zoqQrs\n",
        "\" target=\"\"><img src=\"http://img.youtube.com/vi/wYjp2zoqQrs/0.jpg\" \n",
        "alt=\"\" width=\"240\" height=\"180\" border=\"10\" /></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J-sgWRLxLwW",
        "colab_type": "text"
      },
      "source": [
        "## Imports and Data\n",
        "\n",
        "Lets start by importing the packages and data involved in this lab. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJtPfP8A8O24",
        "colab_type": "code",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "15edf358-f764-4a64-ccb8-601f9900cee6"
      },
      "source": [
        "# Import the packages for this lab\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Import linear regression models\n",
        "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV\n",
        "\n",
        "# Import logistic regression models\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "\n",
        "# Import confusion matrix function from sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load all the data required for this lab\n",
        "df = pd.read_csv('https://docs.google.com/uc?export=download&id=1qCI6BGZpgenI0lLcBuJHTEst5PTIqXov')\n",
        "df.head()  # Visualize small portion of the data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SeasonEnd</th>\n",
              "      <th>Team</th>\n",
              "      <th>W</th>\n",
              "      <th>PTS</th>\n",
              "      <th>oppPTS</th>\n",
              "      <th>FG</th>\n",
              "      <th>FGA</th>\n",
              "      <th>2P</th>\n",
              "      <th>2PA</th>\n",
              "      <th>3P</th>\n",
              "      <th>3PA</th>\n",
              "      <th>FT</th>\n",
              "      <th>FTA</th>\n",
              "      <th>ORB</th>\n",
              "      <th>DRB</th>\n",
              "      <th>AST</th>\n",
              "      <th>STL</th>\n",
              "      <th>BLK</th>\n",
              "      <th>TOV</th>\n",
              "      <th>Conference</th>\n",
              "      <th>Playoffs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2014</td>\n",
              "      <td>Philadelphia 76ers</td>\n",
              "      <td>19</td>\n",
              "      <td>8155</td>\n",
              "      <td>9012</td>\n",
              "      <td>3108</td>\n",
              "      <td>7150</td>\n",
              "      <td>2531</td>\n",
              "      <td>5303</td>\n",
              "      <td>577</td>\n",
              "      <td>1847</td>\n",
              "      <td>1362</td>\n",
              "      <td>1918</td>\n",
              "      <td>949</td>\n",
              "      <td>2556</td>\n",
              "      <td>1791</td>\n",
              "      <td>765</td>\n",
              "      <td>330</td>\n",
              "      <td>1384</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2004</td>\n",
              "      <td>Detroit Pistons</td>\n",
              "      <td>54</td>\n",
              "      <td>7388</td>\n",
              "      <td>6909</td>\n",
              "      <td>2747</td>\n",
              "      <td>6314</td>\n",
              "      <td>2414</td>\n",
              "      <td>5346</td>\n",
              "      <td>333</td>\n",
              "      <td>968</td>\n",
              "      <td>1561</td>\n",
              "      <td>2074</td>\n",
              "      <td>1014</td>\n",
              "      <td>2492</td>\n",
              "      <td>1702</td>\n",
              "      <td>659</td>\n",
              "      <td>570</td>\n",
              "      <td>1241</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2004</td>\n",
              "      <td>San Antonio Spurs</td>\n",
              "      <td>57</td>\n",
              "      <td>7501</td>\n",
              "      <td>6909</td>\n",
              "      <td>2842</td>\n",
              "      <td>6434</td>\n",
              "      <td>2434</td>\n",
              "      <td>5294</td>\n",
              "      <td>408</td>\n",
              "      <td>1140</td>\n",
              "      <td>1409</td>\n",
              "      <td>2069</td>\n",
              "      <td>1029</td>\n",
              "      <td>2669</td>\n",
              "      <td>1676</td>\n",
              "      <td>661</td>\n",
              "      <td>537</td>\n",
              "      <td>1203</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2004</td>\n",
              "      <td>Indiana Pacers</td>\n",
              "      <td>61</td>\n",
              "      <td>7493</td>\n",
              "      <td>7021</td>\n",
              "      <td>2753</td>\n",
              "      <td>6322</td>\n",
              "      <td>2304</td>\n",
              "      <td>5041</td>\n",
              "      <td>449</td>\n",
              "      <td>1281</td>\n",
              "      <td>1538</td>\n",
              "      <td>2014</td>\n",
              "      <td>965</td>\n",
              "      <td>2452</td>\n",
              "      <td>1774</td>\n",
              "      <td>726</td>\n",
              "      <td>411</td>\n",
              "      <td>1182</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1997</td>\n",
              "      <td>Cleveland Cavaliers</td>\n",
              "      <td>42</td>\n",
              "      <td>7173</td>\n",
              "      <td>7022</td>\n",
              "      <td>2704</td>\n",
              "      <td>5972</td>\n",
              "      <td>2221</td>\n",
              "      <td>4688</td>\n",
              "      <td>483</td>\n",
              "      <td>1284</td>\n",
              "      <td>1282</td>\n",
              "      <td>1773</td>\n",
              "      <td>909</td>\n",
              "      <td>2159</td>\n",
              "      <td>1714</td>\n",
              "      <td>655</td>\n",
              "      <td>315</td>\n",
              "      <td>1188</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   SeasonEnd                 Team   W   PTS  ...  BLK   TOV  Conference  Playoffs\n",
              "0       2014   Philadelphia 76ers  19  8155  ...  330  1384           0         0\n",
              "1       2004      Detroit Pistons  54  7388  ...  570  1241           0         1\n",
              "2       2004    San Antonio Spurs  57  7501  ...  537  1203           1         1\n",
              "3       2004       Indiana Pacers  61  7493  ...  411  1182           0         1\n",
              "4       1997  Cleveland Cavaliers  42  7173  ...  315  1188           0         0\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JFxtBtoUr8r",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The columns in the dataset should be as follows (21 columns total):\n",
        "\n",
        "* **SeasonEnd**: season of play\n",
        "* **Team**: team name (categorical)\n",
        "* **W**: wins\n",
        "* **PTS**: points scored over the season\n",
        "* **oppPTS**: points scored on the team over the season\n",
        "* **FG**: field goals made\n",
        "* **FGA**: field goals attempted\n",
        "* **2P**: 2-point field goals made\n",
        "* **2PA**: 2-point field goals attempted\n",
        "* **3P**: 3-point field goals made\n",
        "* **3PA**: 3-point field goals attempted\n",
        "* **FT**: Free throws made\n",
        "* **FTA**: Free throws attempted\n",
        "* **ORB**: Offensive rebounds made\n",
        "* **DRB**: Defensive rebounds made\n",
        "* **AST**: Assists made\n",
        "* **STL**: Steals made\n",
        "* **BLK**: Blocks made\n",
        "* **TOV**: Turnovers made\n",
        "* **Conference**: NBA conference that team plays is (0 = eastern, 1 = western)\n",
        "* **Playoffs**: Whether or not team made playoffs (0 = missed, 1 = made)\n",
        "\n",
        "Before we do anything, we should always try some exploratory data analysis to better understand what we are dealing with. Answer the questions below.\n",
        "\n",
        "### Exercise\n",
        "\n",
        "1. Is there any missing data?\n",
        "2. Which team won the most games in a season?\n",
        "3. Which team was the best defensive team (based on points scored against the team)?\n",
        "4. Aside from playoffs, which feature is the most correlated with wins?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsphPBZBQ6Zd",
        "colab_type": "code",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f181cbd-0217-4afc-fb9f-932320cb9460"
      },
      "source": [
        "# Question 1\n",
        "\n",
        "# Write your code here.\n",
        "\n",
        "# -------------------\n",
        "\n",
        "print(f'There are {df.isnull().sum().sum()} missing data points')\n",
        "\n",
        "# -------------------"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 0 missing data points\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DGdvAdGYsT-",
        "colab_type": "code",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "182f3b65-d419-4fa1-946b-c4aba1ef102c"
      },
      "source": [
        "# Question 2\n",
        "\n",
        "# Write your code here.\n",
        "\n",
        "# -------------------\n",
        "\n",
        "df.loc[df.W.idxmax()].Team\n",
        "\n",
        "# -------------------"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Golden State Warriors'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyJefMh1Zap1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "aae45d8e-3b0d-43ee-c6f0-90b4c88421df"
      },
      "source": [
        "# Question 3\n",
        "\n",
        "# Write your code here.\n",
        "\n",
        "# -------------------\n",
        "\n",
        "df.loc[df.oppPTS.idxmin()].Team\n",
        "\n",
        "# -------------------\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Detroit Pistons'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxdpRx8oZtnt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "300c600f-1222-4aaf-bd3f-66a82c51b504"
      },
      "source": [
        "# Question 4\n",
        "\n",
        "# Write your code here.\n",
        "\n",
        "# -------------------\n",
        "\n",
        "df.corr().W.drop(['W', 'Playoffs']).abs().idxmax()\n",
        "\n",
        "# -------------------"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'DRB'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-J38pOLabG9",
        "colab_type": "text"
      },
      "source": [
        "## Feature engineering\n",
        "\n",
        "In this section, we're going to use our findings in the previous exercise to make a new feature out of the existing data using a process called feature engineering. Consider that (in addition to being great overall team) the Golden State Warriors (2016) and Detroit Pistons (2004) are widely regarded as the best offensive and defensive team of our time, respectively. If you look at the full correlation matrix it seems like PTS and oppPTS are correlated with winning, but not as much as DRB. This should seem surprising as you would expect that the points scored and allowed should be the best indicator of whether a team wins a game. \n",
        "\n",
        "By applying some domain knowledge to this problem, however, we might reason that it is not the individual two features that affect how often you win but rather the combination of those features. In the code below, we create a new feature called \"diffPTS\" (to represent difference in points scored by a team and points scored against a team), and then we check how it correlates with wins.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g1yj7_DfVZc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d34a0dd4-0d2b-43e0-a633-3b0a4829be7c"
      },
      "source": [
        "# Make new column in df\n",
        "df['dffPTS'] = df.PTS - df.oppPTS  # new column data\n",
        "dffPTS_corr = df[['dffPTS', 'W']].corr().iloc[0,1]  # get correlation with wins\n",
        "\n",
        "# Get highest correlation coefficient from original data\n",
        "highest_orig_corr = df.corr().W.drop(['W', 'Playoffs', 'dffPTS']).abs().max()\n",
        "\n",
        "print(f'Using only the original features the highest correlation with wins was {highest_orig_corr:.3f},\\n \\\n",
        "but our new feature had a much higher correlation with wins of {dffPTS_corr:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using only the original features the highest correlation with wins was 0.411,\n",
            " but our new feature had a much higher correlation with wins of 0.971\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrodQtgsxF3P",
        "colab_type": "text"
      },
      "source": [
        "By engineering this new feature dffPTS we have a feature that correlates with wins more than any of the original features! As a very brief aside (will not be tested), the `:.3f` in the print statment is called a format specifier, and the logic is explained [here.](https://www.python.org/dev/peps/pep-0498/#format-specifiers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYaGkyBAiUqd",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression\n",
        "\n",
        "Linear regression can be used when the target (i.e., what you're trying to predict) is a continuous variable that can take any values from minus infinity to positive infinity. Let $y$ represent the target and $x_1,x_2,\\dots,x_K$ represent $K$ features (i.e., what you're using to predict the target). \n",
        "\n",
        "We use linear regression to find the best coefficients $\\beta_0,\\beta_1,\\dots,\\beta_K$, such that our predictions,\n",
        "\n",
        "$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_Kx_K,$\n",
        "\n",
        "are as close as possible to the true target values, $\\hat{y}$. We define the error (i.e., residual) for a given observation to be $(\\hat{y} -y)$. Then we choose our regression coefficients to minimize the mean squared error (MSE) between the true target values and the predicted target values across all data points. For example, suppose we observe $n$ data points of the form $(y_i,x_{i1},x_{i2},\\dots,x_{iK}), \\;i=1,\\dots,n$, then we would solve this optimization problem\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\min_{\\beta_0,\\beta_1,\\dots,\\beta_K} \\quad  & \\frac{1}{n}\\sum_{i=1}^n \\left( \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_Kx_{iK} - y_i \\right)^2.     \\quad\\quad\\quad (1)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We denote the coefficients that minimize (1) by $\\hat{\\beta}_0,\\hat{\\beta}_1,\\dots,\\hat{\\beta}_K$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB2b2KyAxyGZ",
        "colab_type": "text"
      },
      "source": [
        "## Coding up the model\n",
        "\n",
        "In this course, we will mostly use the `scikit-learn` package to build predictive models. In `scikit-learn`, every model is defined as a class object that contains (amongst others), several standard methods:\n",
        "\n",
        "* **fit()**: Takes training features and target as input and uses that to fit the model (i.e., find the best parameters for a model).\n",
        "* **predict()**: Takes testing features as input and uses that to predict target values (i.e., use the model).\n",
        "* **score()**: Takes testing features and target as input and uses that to measure the error (i.e., evaluate model performance).\n",
        "\n",
        "\n",
        "We will now build a linear regression model to predict the total number of wins using all of the numerical features. When building any model, we make sure that we split the data into reasonable training and testing sets. We then use  [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) and fit our model on the training data and predict on the target data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F7TDV8MgxXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data into training and testing sets\n",
        "df_train = df[df.SeasonEnd<2012]\n",
        "df_test = df[df.SeasonEnd>=2012]\n",
        "\n",
        "# Define features to remove from the feature data (i.e., X)\n",
        "drop_for_X = ['W','Team','SeasonEnd','Playoffs']\n",
        "\n",
        "# Partition the training data into features and target\n",
        "X_train = df_train.drop(columns=drop_for_X)\n",
        "y_train = df_train.W\n",
        "\n",
        "# Fit the model\n",
        "linreg = LinearRegression()\n",
        "linreg.fit(X_train, y_train)\n",
        "\n",
        "# Partition the testing data into features and target\n",
        "X_test = df_test.drop(columns=drop_for_X)\n",
        "y_test = df_test.W\n",
        "\n",
        "# Predict the number of wins\n",
        "y_test_predictions = linreg.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brm2gfODmAAL",
        "colab_type": "text"
      },
      "source": [
        "In the codeblock above, we used the `predict()` method to predict $y$ for each data point (i.e., row) in `X_test`. To determine how _good_ those predictions were you should use the score function, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDMIWnXol_Hi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c52649e-e475-465a-9610-18e50be6fe8b"
      },
      "source": [
        "train_score = linreg.score(X_train, y_train)\n",
        "test_score = linreg.score(X_test, y_test)\n",
        "\n",
        "print(f'The train score is {train_score:.3f} and the test score is {test_score:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The train score is 0.944 and the test score is 0.888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsXhqUp9oe1M",
        "colab_type": "text"
      },
      "source": [
        "The \"score\" for linear regression is the _coefficient of determination_ (i.e., $R^2$). In the space above, we calculated the score of the training and testing data, and we found that the model performed better on the training data ($R^2$=0.944) than it did on the testing data ($R^2=0.888). In general the model will perform better on the data it was trained on than data it is seeing for the first time (i.e., testing set). Although most (if not all) models in `scikit-learn` have a score method, the score that `score` evaluates depends on the model. Please keep this in mind when you write a report. \n",
        "\n",
        "In a `LinearRegression` model, the `coef_` attribute returns the values for $\\beta$ and the `intercept_` attribute stores the value fo $\\beta_0$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jLoNgNSoRdJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "afb40ad1-bb29-471b-b580-0a122d1242b9"
      },
      "source": [
        "betas = pd.Series(linreg.coef_, index=X_train.columns)\n",
        "betas = betas.append(pd.Series({\"Intercept\": linreg.intercept_}))\n",
        "print(betas)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PTS          -1.032659e+10\n",
            "oppPTS       -4.945743e+09\n",
            "FG            3.136950e+10\n",
            "FGA           1.995394e+10\n",
            "2P           -8.248390e+08\n",
            "2PA          -1.995394e+10\n",
            "3P            1.444749e+10\n",
            "3PA          -1.995394e+10\n",
            "FT            1.527233e+10\n",
            "FTA          -1.105242e-03\n",
            "ORB           1.955840e-03\n",
            "DRB           2.634602e-03\n",
            "AST           1.160074e-03\n",
            "STL           1.172673e-03\n",
            "BLK           3.770994e-03\n",
            "TOV          -2.041234e-03\n",
            "Conference    1.902110e-01\n",
            "dffPTS       -4.945743e+09\n",
            "Intercept     3.897555e+01\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLysXYhKpg9a",
        "colab_type": "text"
      },
      "source": [
        "You should observe that some of the beta coefficients (e.g., for `oppPTS` and `dffPTS`) are identical, which is the sign of a problem. The problem is that many of the features are highly correlated with each other, and we are therefore violating the assumption of no multi-collinearity!\n",
        "\n",
        "### Exercise\n",
        "1. In the space below, plot the correlation matrix. Which two features are the most correlated?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kTsqB-To1IO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 999
        },
        "outputId": "fc6e0a1c-7849-4f74-9705-3a5ec23f027c"
      },
      "source": [
        "# Write your code here.\n",
        "\n",
        "# -------------------\n",
        "\n",
        "sns.heatmap(df_train.corr())\n",
        "df_train.corr()[df_train.corr().abs() > 0.99]\n",
        "\n",
        "# -------------------"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SeasonEnd</th>\n",
              "      <th>W</th>\n",
              "      <th>PTS</th>\n",
              "      <th>oppPTS</th>\n",
              "      <th>FG</th>\n",
              "      <th>FGA</th>\n",
              "      <th>2P</th>\n",
              "      <th>2PA</th>\n",
              "      <th>3P</th>\n",
              "      <th>3PA</th>\n",
              "      <th>FT</th>\n",
              "      <th>FTA</th>\n",
              "      <th>ORB</th>\n",
              "      <th>DRB</th>\n",
              "      <th>AST</th>\n",
              "      <th>STL</th>\n",
              "      <th>BLK</th>\n",
              "      <th>TOV</th>\n",
              "      <th>Conference</th>\n",
              "      <th>Playoffs</th>\n",
              "      <th>dffPTS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>SeasonEnd</th>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>W</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PTS</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oppPTS</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FG</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FGA</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2P</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2PA</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3P</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.994511</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3PA</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.994511</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FT</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FTA</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ORB</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DRB</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AST</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STL</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BLK</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TOV</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Conference</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Playoffs</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dffPTS</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            SeasonEnd    W  PTS  oppPTS  ...  TOV  Conference  Playoffs  dffPTS\n",
              "SeasonEnd         1.0  NaN  NaN     NaN  ...  NaN         NaN       NaN     NaN\n",
              "W                 NaN  1.0  NaN     NaN  ...  NaN         NaN       NaN     NaN\n",
              "PTS               NaN  NaN  1.0     NaN  ...  NaN         NaN       NaN     NaN\n",
              "oppPTS            NaN  NaN  NaN     1.0  ...  NaN         NaN       NaN     NaN\n",
              "FG                NaN  NaN  NaN     NaN  ...  NaN         NaN       NaN     NaN\n",
              "FGA               NaN  NaN  NaN     NaN  ...  NaN         NaN       NaN     NaN\n",
              "2P                NaN  NaN  NaN     NaN  ...  NaN         NaN       NaN     NaN\n",
              "2PA               NaN  NaN  NaN     NaN  ...  NaN         NaN       NaN     NaN\n",
              "3P                NaN  NaN  NaN     NaN  ...  NaN         NaN       NaN     NaN\n",
              "3PA               NaN  NaN  NaN     NaN  ...  NaN         NaN       NaN     NaN\n",
              "FT                NaN  NaN  NaN     NaN  ...  NaN         NaN       NaN     NaN\n",
              "FTA               NaN  NaN  NaN     NaN  ...  NaN         NaN       NaN     NaN\n",
              "ORB               NaN  NaN  NaN     NaN  ...  NaN         NaN       NaN     NaN\n",
              "DRB               NaN  NaN  NaN     NaN  ...  NaN         NaN       NaN     NaN\n",
              "AST               NaN  NaN  NaN     NaN  ...  NaN         NaN       NaN     NaN\n",
              "STL               NaN  NaN  NaN     NaN  ...  NaN         NaN       NaN     NaN\n",
              "BLK               NaN  NaN  NaN     NaN  ...  NaN         NaN       NaN     NaN\n",
              "TOV               NaN  NaN  NaN     NaN  ...  1.0         NaN       NaN     NaN\n",
              "Conference        NaN  NaN  NaN     NaN  ...  NaN         1.0       NaN     NaN\n",
              "Playoffs          NaN  NaN  NaN     NaN  ...  NaN         NaN       1.0     NaN\n",
              "dffPTS            NaN  NaN  NaN     NaN  ...  NaN         NaN       NaN     1.0\n",
              "\n",
              "[21 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOZUiIS1vEyd",
        "colab_type": "text"
      },
      "source": [
        "___\n",
        "**Question 1 answer:** a quick spot-check shows that 3P and 3PA have a correlation of 0.99\n",
        "___\n",
        "\n",
        "## Correlated features\n",
        "Below we define a function and describe it with a [_doc string_](https://www.datacamp.com/community/tutorials/docstrings-python?utm_source=adwords_ppc&utm_campaignid=898687156&utm_adgroupid=48947256715&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=332602034352&utm_targetid=aud-392016246653:dsa-429603003980&utm_loc_interest_ms=&utm_loc_physical_ms=9061009&gclid=Cj0KCQjwy8f6BRC7ARIsAPIXOjhfMN9Oj6QCqNfMMDgZYqFFwp7yv6P9V_agl4PtR3O6o95GidfErGYaAsSCEALw_wcB) (i.e., the string at the begining of the function). A doc string should be present at the begining of most functions contained in reputable python packages.  We'll use this function to go through every feature and count the number of times this feature is correlated or anti-correlated with another feature by more than `threshold`. We set the default value of `threshold` to 0.7.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5asXhxwqWl2",
        "colab_type": "code",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "e5f4bd31-9886-4c98-dae3-461c8d0d223d"
      },
      "source": [
        "# Define a function\n",
        "def print_heavily_correlated_features(df, threshold=0.7):\n",
        "  \"\"\"\n",
        "  For each feature in \"df\", this function counts the number of features that \n",
        "  have a correlation coefficient with that is higher than \"threshold\".\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "    df: pandas DataFrame type \n",
        "        Contains the features for several data points\n",
        "    threshold: float type \n",
        "        he threshold for which \"heavily correlated\" is defined.\n",
        "  \"\"\"\n",
        "\n",
        "  corr = df.corr().abs()  # calculate the correlation matrix\n",
        "  corr = corr[corr > threshold]  # a mask of features that are heavily correlated\n",
        "\n",
        "  # Print out the \"heavily correlated\" counts\n",
        "  print(corr.count().sort_values(ascending=False) - 1)  \n",
        "\n",
        "# Execute the function\n",
        "print_heavily_correlated_features(df_train)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2P            9\n",
            "2PA           9\n",
            "SeasonEnd     7\n",
            "FG            7\n",
            "PTS           6\n",
            "FGA           6\n",
            "oppPTS        5\n",
            "3P            4\n",
            "3PA           4\n",
            "AST           3\n",
            "ORB           3\n",
            "W             2\n",
            "dffPTS        2\n",
            "Playoffs      2\n",
            "FTA           1\n",
            "TOV           1\n",
            "FT            1\n",
            "DRB           0\n",
            "STL           0\n",
            "BLK           0\n",
            "Conference    0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewnyfp_NzV1e",
        "colab_type": "text"
      },
      "source": [
        "### Exercise\n",
        "1. Why did we run the above function on df_train rather than df?\n",
        "\n",
        "2. In the codeblock below, we created a new DataFrame called `df_reduced`. Use it to fit a better regression model by following these steps: \n",
        "i)  From `df_reduced` remove the 5 features most heavily susceptible to multi-collinearity (i.e., 2P, 2PA, FG, PTS, FGPA).  \n",
        "ii) Then, fit a new linear regression model `linreg_reduced` to this data set using the same train-test split.  \n",
        "iii) Report the score and the beta coefficients.\n",
        "___\n",
        "**Question 1 answer:**\n",
        "We should be doing all analysis with respect to model fitting on our training set. The test set is used only to evaluate model quality.\n",
        "___\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNmcVNYHqYZZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "d8038bbf-8739-4f91-8cbf-efa99c12441f"
      },
      "source": [
        "# We have made a copy of the data frame\n",
        "df_reduced = df.copy()\n",
        "\n",
        "# REMEMBER: remove drop the features listed in drop_for_X from X_train and X_test\n",
        "\n",
        "# Write your code here.\n",
        "\n",
        "# -------------------\n",
        "\n",
        "# Step i starts here\n",
        "# Remove the correlated columns\n",
        "columns_to_remove = [ '2P', '2PA', 'FG', 'PTS', 'FGA']\n",
        "df_reduced = df_reduced.drop(columns=columns_to_remove)\n",
        "\n",
        "# Step ii starts here\n",
        "# Split the data into training and testing sets\n",
        "df_train = df_reduced[df_reduced.SeasonEnd<2012]\n",
        "df_test = df_reduced[df_reduced.SeasonEnd>=2012]\n",
        "\n",
        "# Partition the training data into features and target\n",
        "X_train = df_train.drop(columns=drop_for_X)\n",
        "y_train = df_train.W\n",
        "\n",
        "# Fit the model\n",
        "linreg_reduced = LinearRegression()\n",
        "linreg_reduced.fit(X_train, y_train)\n",
        "\n",
        "# Partition the testing data into features and target\n",
        "X_test = df_test.drop(columns=drop_for_X)\n",
        "y_test = df_test.W\n",
        "\n",
        "# Step iii starts here\n",
        "# calculate the score\n",
        "train_score = linreg_reduced.score(X_train, y_train)\n",
        "test_score = linreg_reduced.score(X_test, y_test)\n",
        "print(f'The train score is {train_score:.3f} and the test score is {test_score:.3f}')\n",
        "\n",
        "# calculate betas\n",
        "betas = pd.Series(linreg_reduced.coef_, index=X_train.columns)\n",
        "betas = betas.append(pd.Series({\"Intercept\": linreg_reduced.intercept_}))\n",
        "print(betas)\n",
        "\n",
        "# -------------------\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The train score is 0.944 and the test score is 0.932\n",
            "oppPTS        -0.000136\n",
            "3P            -0.007007\n",
            "3PA            0.002766\n",
            "FT             0.000195\n",
            "FTA           -0.000227\n",
            "ORB           -0.000462\n",
            "DRB            0.001097\n",
            "AST            0.001251\n",
            "STL           -0.000716\n",
            "BLK            0.003887\n",
            "TOV           -0.000670\n",
            "Conference     0.203804\n",
            "dffPTS         0.032004\n",
            "Intercept     37.067376\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ2fOwIq16Fd",
        "colab_type": "text"
      },
      "source": [
        "You should notice that your model now gives a much higher score on the test set! Furthermore, the test score is more or less the same as the train score.\n",
        "\n",
        "## Overfitting\n",
        "\n",
        "In the first model that used all of the features (i.e., `linreg`), the train score was much higher than the test score. This means that the model was _overfitting_ to the training set. Overfitting occurs when the model identifies relationships that are unique to the training set and do not generalize to unseen (testing) data. Note that in the model with fewer features (i.e., the reduced model) actually performed much better on the out-of-sample data because it did not overfit. \n",
        "\n",
        "Multi-collinearity can cause overfitting, but there are other causes that we won't cover in this lab including:\n",
        "\n",
        "1. Small dataset\n",
        "2. Complex model \n",
        "3. Hidden biases in the data (e.g., multi-collinearity)\n",
        "\n",
        "Another important observation is that the $\\beta$ values make a lot more sense now in terms of how they would affect the number of wins of a given team.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvYb2dEc0h1D",
        "colab_type": "text"
      },
      "source": [
        "### Exercise\n",
        "1. How many more three point shots does a team need to score in a given season in order to win 1 additional game?\n",
        "\n",
        "2. According to your model, does having more offensive rebounds help or hurt a team's chances of winning games? Does this make sense? Why or why not?\n",
        "\n",
        "___\n",
        "**Question 1 answer**\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "1 &= \\beta_{3P}x + \\beta_{3PA}x + \\beta_{dffPTS} (3x)\\\\\n",
        "x &= 10.90\n",
        "\\end{align}\n",
        "$$\n",
        "___\n",
        "**Question 2 answer**:\n",
        "\n",
        "Offensive rebounds seem to hurt the chances of winning. This suggests that the model is still somewhat failing to characterize the effect of the features.\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxSmh7uk7vzh",
        "colab_type": "text"
      },
      "source": [
        "# Regularization\n",
        "\n",
        "Previously, we saw that the model was overfitting because it had too many features. To reduce overfitting, we used our domain knowledge to remove features from the model using a process called _manual feature selection_. If we have a poor understanding of the problem domain and/or we suspect our model is overfitting then doing manual feature selection is difficult. \n",
        "\n",
        "A more automated way to reduce overfitting that requires no domain knowledge is _regularization_. This technique is especially useful in applications with a _relatively large_ number of features ($K$) compared to the number of samples ($N$) in the training dataset. A __very rough__ rule of thumb is that you should have no more than $\\sqrt N$ features in your model (i.e., $K\\le\\sqrt N$). Regularization is a very general operation that can be applied to most models. In this section, we'll explain regularization in the context of linear regression.\n",
        "\n",
        "In regularized linear regression, we add the term $\\frac{1}{C}\\sum_{k=1}^K |\\beta_k|^p$ to the objective of Equation (1), where $p$ is a number that denotes the type of regularization (e.g., L1, L2) and $\\frac{1}{C}$ is a parameter learned by the model (or occasionally selected by the user). The parameter $\\frac{1}{C}$ controls the _strength of regularization_. Intuitively, you can think of regularization as a process that penalizes non-zero regression coefficients (i.e., $\\beta > 0$), but keep in mind that different types of regularization produce different results. There are two common types of regularization:\n",
        "\n",
        " 1. __L1-regularization ($p=1$):__ Also called _LASSO_ regression, is frequently used for feature selection because it implicitly produces a sparse solution, i.e., only a small portion of the regression coefficients will be non-zero. Using L1-regularization, we can rewrite (1) as             \n",
        "    \\begin{aligned} \n",
        "        \\min_{\\beta_0,\\beta_1,\\dots,\\beta_K} \\quad  & \\frac{1}{n}\\sum_{i=1}^n \\left( \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_Kx_{iK} - y_i \\right)^2 + \\frac{1}{C} \\sum_{k=1}^K |\\beta_k|.\n",
        "    \\end{aligned}\n",
        "               \n",
        " 2.  __L2-regularization ($p=2$):__ Also called _ridge_ regression, is frequently used to produce a model that tries to use all of the available features. Using L2-regularization, we can rewrite (1) as\n",
        "        \\begin{aligned}\n",
        "            \\min_{\\beta_0,\\beta_1,\\dots,\\beta_K} \\quad  & \\frac{1}{n}\\sum_{i=1}^n \\left( \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_Kx_{iK} - y_i \\right)^2 + \\frac{1}{C} \\sum_{k=1}^K |\\beta_k|^2.\n",
        "        \\end{aligned}\n",
        "        \n",
        "\n",
        "Both methods reduce the sum of coefficients and prevent overfitting, however, in general each methods has different use cases. Also, note that L2-regularization is more computationally efficient than L1-regularization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QckZK337QNN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Cross-validation\n",
        "_Cross-validation_ is a process for selecting model hyper-parameters (e.g., $C$) that generalize to out-of-sample data. Cross-validation is performed in $m$ rounds. In each round the training set is split into two subsets (1) a subset that the model is trained on, and (2) a subset that the model is evaluated on. After the rounds are complete, an aggregate measure of performance (e.g., average score) is used to measure model performance. \n",
        "\n",
        "We can use cross validation to search for the best parameter out of a list. For example, we could look for the best parameter $C$ for a regression model out of the list [0.1, 1, 10, 100]. Cross-validation would perform $m$ rounds ($m=4$ in this example) for each element in that list, and give us the best performing model (i.e., the best available $C$). \n",
        "\n",
        "Let's try building a new linear regression model with regularization. We'll pretend that we know nothing about basketball. In this case we use the [`LassoCV` model from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV) to build an L1-regularized linear regression model. The `LassoCV` model automatically applies cross-validation to find the best value for $C$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA1hpWmf1i_J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "df9cb634-fc1d-4941-a81e-a1433604c627"
      },
      "source": [
        "# Let's go back to the original data set without the removed features\n",
        "# Split the data into training and testing sets\n",
        "df_train = df[df_reduced.SeasonEnd<2012]\n",
        "df_test = df[df.SeasonEnd>=2012]\n",
        "\n",
        "# Partition the training data into features and target\n",
        "X_train = df_train.drop(columns=['W','Team','SeasonEnd'])\n",
        "y_train = df_train.W\n",
        "\n",
        "# Fit the model\n",
        "lasso = LassoCV()\n",
        "lasso.fit(X_train, y_train)\n",
        "\n",
        "# Partition the testing data into features and target\n",
        "X_test = df_test.drop(columns=['W','Team','SeasonEnd'])\n",
        "y_test = df_test.W\n",
        "\n",
        "# calculate the score\n",
        "train_score = lasso.score(X_train, y_train)\n",
        "test_score = lasso.score(X_test, y_test)\n",
        "print(f'The train score is {train_score:.3} and the test score is {test_score:.3}')\n",
        "\n",
        "# calculate betas\n",
        "betas = pd.Series(lasso.coef_, index=X_train.columns)\n",
        "betas = betas.append(pd.Series({\"Intercept\": lasso.intercept_}))\n",
        "print(betas)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The train score is 0.942 and the test score is 0.949\n",
            "PTS            0.000000\n",
            "oppPTS        -0.000000\n",
            "FG             0.000000\n",
            "FGA           -0.000000\n",
            "2P            -0.000000\n",
            "2PA           -0.000000\n",
            "3P             0.000000\n",
            "3PA            0.000000\n",
            "FT            -0.000000\n",
            "FTA           -0.000000\n",
            "ORB           -0.000000\n",
            "DRB            0.000000\n",
            "AST            0.000000\n",
            "STL           -0.000000\n",
            "BLK            0.000000\n",
            "TOV           -0.000000\n",
            "Conference     0.000000\n",
            "Playoffs       0.000000\n",
            "dffPTS         0.032282\n",
            "Intercept     41.000000\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNs2xQTp4Qwi",
        "colab_type": "text"
      },
      "source": [
        "You should observe that the LASSO model performs better than our manually reduced model! Remember that the model automatically performed cross-validation to get the best regularization strength. Also notice that the beta values are approximately zero for nearly every feature except for dffPTS. \n",
        "\n",
        "### Exercise\n",
        "1. Use the attribute `alpha_` to find the regularization strength (i.e., $\\frac{1}{C}$) for the model `lasso`. Keep in mind that the attribute `alpha_` returns $\\alpha = \\frac{1}{C}$.\n",
        "\n",
        "2. Would you prefer this model (`lasso`) or the one with manual feature selection (`linreg_reduced`) to better understand what it takes to win?\n",
        "\n",
        "3. Try building a LASSO model with the reduced data set (`df_reduced`). Do you expect it to differ from the above LASSO model? How does it perform?\n",
        "\n",
        "4. In the space below, fit a ridge regression model on the original dataset (`df`) using [`RidgeCV` from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV), and call that model `ridge`. Compare the test score and the betas with the `lasso` model. How do the models differ with respect to score and betas? Which model would you use to understand the effects of features on winning?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OclDpbSW4yhO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "abee4d62-7df8-494b-ea30-65dea3372d25"
      },
      "source": [
        "# Question 1\n",
        "\n",
        "# Write your code here.\n",
        "\n",
        "# -------------------\n",
        "\n",
        "print(f'The regularization strength is 1/C = {lasso.alpha_:.3f}')\n",
        "\n",
        "# -------------------\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The regularization strength is 1/C = 43.726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9meQRgBKwz4",
        "colab_type": "text"
      },
      "source": [
        "___\n",
        "**Question 2 answer:**\n",
        "\n",
        "I would use the LASSO model because everything boils down to a single very interpretable feature. This is great because you don't need to worry about how each feature depends on each other (e.g., how scoring a 3 point shot will change other features).\n",
        "___\n",
        "**Question 3 answer:**\n",
        "\n",
        "It builds an identical model. This is expected because the features that we removed from `df` to make `df_reduced` had zero weight in the `lasso` model. \n",
        "___\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97n6tSdwJrSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "e1e7997d-b4b2-4cc6-d19f-9ef21c1395f4"
      },
      "source": [
        "# Question 4 code\n",
        "\n",
        "# Write your code here.\n",
        "\n",
        "# -------------------\n",
        "\n",
        "# Let's go back to the original data set without the removed features\n",
        "# Split the data into training and testing sets\n",
        "df_train = df[df.SeasonEnd<2012]\n",
        "df_test = df[df.SeasonEnd>=2012]\n",
        "\n",
        "# Partition the training data into features and target\n",
        "X_train = df_train.drop(columns=drop_for_X)\n",
        "y_train = df_train.W\n",
        "\n",
        "# Fit the model\n",
        "ridge = RidgeCV()\n",
        "ridge.fit(X_train, y_train)\n",
        "\n",
        "# Partition the testing data into features and target\n",
        "X_test = df_test.drop(columns=drop_for_X)\n",
        "y_test = df_test.W\n",
        "\n",
        "# calculate the score\n",
        "train_score = ridge.score(X_train, y_train)\n",
        "test_score = ridge.score(X_test, y_test)\n",
        "print(f'The train score is {train_score:.3} and the test score is {test_score:.3}')\n",
        "\n",
        "# calculate betas\n",
        "betas = pd.Series(ridge.coef_, index=X_train.columns)\n",
        "betas = betas.append(pd.Series({\"Intercept\": ridge.intercept_}))\n",
        "print(betas)\n",
        "\n",
        "# -------------------"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The train score is 0.944 and the test score is 0.89\n",
            "PTS            0.008856\n",
            "oppPTS        -0.011170\n",
            "FG             0.003202\n",
            "FGA           -0.000576\n",
            "2P             0.003958\n",
            "2PA           -0.001742\n",
            "3P            -0.000756\n",
            "3PA            0.001166\n",
            "FT             0.003209\n",
            "FTA           -0.001101\n",
            "ORB            0.001959\n",
            "DRB            0.002638\n",
            "AST            0.001155\n",
            "STL            0.001179\n",
            "BLK            0.003776\n",
            "TOV           -0.002044\n",
            "Conference     0.180656\n",
            "dffPTS         0.020026\n",
            "Intercept     38.852343\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67N_ItrIAS5f",
        "colab_type": "text"
      },
      "source": [
        "___\n",
        "**Question 4 answer:**\n",
        "\n",
        "The `ridge` model is actually quite informative even though it performs worse than `lasso` on the test set. Notice that the signs on all of the features make intuitive sense. oppPTS and TOV should be negative as those things are generally bad for a team. FGA, 2PA, FTA are all negative, but FG, 2P, FT are positive, meaning that shooting more isn't necessarily good but scoring more is. Unfortunately, this intuition doesn't extend to the 3P feature, which is still negative.\n",
        "\n",
        "Overall, the `lasso` model is still the most interpretable model. It simply says, \"score more points than your opponent, and you'll win more games\". In contrast, the `ridge` model just us some more information about what specific actions a team could take to win more games.\n",
        "___\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmW4lnDiKsu9",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "Logistic regression can be used when the target is a categorical variable that  takes values of 0 and 1. Recall that in linear regression, $y$ is modeled as a linear function of $\\mathbf{x}$; in logistic regression, we model the _logit_ or _log-odds ratio_ as a linear function of $\\mathbf{x}$. That is,\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    \\mathrm{logit}(y) = \\log\\left(\\frac{y}{1-y}\\right) = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_K x_K\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We can rewrite the logit equation to obtain the probability,\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "y = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_K x_K)}}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "In order to turn the logit into a prediction, we apply a threshold rule $T(y)$. For example, a threshold rule of $0.5$ corresponds to\n",
        "$$ T(y) = \n",
        "\\begin{cases}\n",
        "     1,& \\text{ if } y \\geq 0.5, \\\\\n",
        "     0,& \\text{ if } y \\leq 0.5.\n",
        "\\end{cases}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Bzfd-eYDIdi",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Evaluating model performance\n",
        "\n",
        "The standard way to score (i.e., evaluate performance) a logistic regression is via mean accuracy. This is equal to the proportion of data points $i = 1, 2, \\dots, n$ for which $T(y_i) = \\hat{y}_i$. We calculate it as \n",
        "$1 - \\frac{\\sum_{i=1}^N |T(y_i) - \\hat{y}_i| }{N}$. We can think of this scoring metric for logistic regression as something that is analogous to the $R^2$ metric for linear regression. \n",
        "\n",
        "For a binary classification problem, although the accuracy of a model is a great measure of its performance, accuracy fails to reveal the type of errors that the model makes. There are two main types of errors: (1) false positives (FP), where a value of $1$ is predicted (i.e., $y = 1$) when the true value is $0$ (i.e., $\\hat{y} = 0$); and (2) false negatives (FN), where a value of $0$ is predicted (i.e., $y = 0$) when the true value is $1$ (i.e., $\\hat{y} = 1$). A correctly classified point is a true positive (TP) or true negative (TN) when a value of $1$ or $0$ is correctly predicted, respectively. There are several common summary statistics that summarize the frequency of different error types that are summarized in the following table.\n",
        "\n",
        "| Statistic                             | Calculation    | Definition |\n",
        "|:--------------------------------------|:--------------:|:-----------|\n",
        "| Recall/Sensitivity/True Positive Rate | TP / (TP + FN) | the proportion of $y_i = 1$ that were correctly labelled |\n",
        "| Fall-out/False Positive Rate          | FP / (FP + TN) | the proportion of $y_i = 0$ that were incorrectly labelled |\n",
        "| Precision/Positive Predictive Value   | TP / (TP + FP) | the proportion of $T(\\hat{y}_i) = 1$ that match the data |\n",
        "| False Omission Rate                   | FN / (FN + TN) | the proportion of $T(\\hat{y}_i) = 0$ that don't match the data |\n",
        "\n",
        "In general, true positive rate (TPR) and false positive rate (FPR) are the most common of the ratios presented above. TPR is a measure of how often we correctly identify when the target is true (i.e., $y = 1$ and $\\hat{y} = 1$), and FPR is a measure of how often we incorrectly identify when the target is true (i.e., $y = 1$ and $\\hat{y}=0$). There are many other metrics that quantify model error, and the best metric to use depends on the application of the model. We can summarize all of this information inside a confusion matrix.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1194/0*wKaznIJzZF54b87B.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "378hdG56DMgP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Coding the model\n",
        "\n",
        "Remember that linear regression predicts continuous target values, and logistic regression predicts target binary values (i.e., categorical). While we used linear regression to predict the total number of games that a team is expected to win in a season, we will now use logistic regression to predict whether a team will make the playoffs or not.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9UMlfW2DN3v",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Exercise\n",
        "1. In the space below, fit a [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model called `logreg` to predict the target (from the column \"Playoffs\" in `df`) using the same features that we used in the linear regression model above. Don't forget to drop the column headings ['W','Team','SeasonEnd', 'Playoffs'], which are stored in the variable `drop_for_X`, from your features (`X_train` and `X_test`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g36DLvvQC4v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "41470b01-fdaa-46c6-e82b-25540663d84c"
      },
      "source": [
        "# Split the data into training and testing sets\n",
        "df_train = df[df.SeasonEnd<2012]\n",
        "df_test = df[df.SeasonEnd>=2012]\n",
        "\n",
        "# Write your code here.\n",
        "\n",
        "# -------------------\n",
        "\n",
        "# Partition the training data into features and target\n",
        "X_train = df_train.drop(columns=drop_for_X)\n",
        "y_train = df_train.Playoffs\n",
        "\n",
        "# Fit the model\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Partition the testing data into features and target\n",
        "X_test = df_test.drop(columns=drop_for_X)\n",
        "y_test = df_test.Playoffs\n",
        "\n",
        "# Predict the number of wins\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# -------------------"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZPRktKl2o_s",
        "colab_type": "text"
      },
      "source": [
        "## Addressing a warning message\n",
        "You should see a message about a `ConvergenceWarning` that explains that the solver (i.e., the optimizer that \"finds\" the coefficients for the model) failed to converge before fitting the iteration limit. This limit is set by the `max_iter` parameter, and by default it is limited to 100 iteration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1vmR6x3Lp53",
        "colab_type": "text"
      },
      "source": [
        "### Exercise\n",
        "1. Make a new model called `logreg_max_iter_5000` by training the model from the code block above for a maximum of 5000 iterations. \n",
        "\n",
        "2. Use the `score` method for logistic regression to evaluate the mean accuracy of `logreg` and `logreg_max_iter_5000`. What model performed best? \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L_JkBlu62qA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "b31cd3a1-050b-4b91-896f-a4ed79a0d968"
      },
      "source": [
        "### Question 1\n",
        "\n",
        "# -------------------\n",
        "\n",
        "# Fit the model\n",
        "logreg_max_iter_5000 = LogisticRegression(max_iter=5000)\n",
        "logreg_max_iter_5000.fit(X_train, y_train)\n",
        "\n",
        "# -------------------"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=5000,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0c92XD_2nQ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "af04ca4f-6772-43af-df4d-b1c61d20e636"
      },
      "source": [
        "#### Question 2\n",
        "\n",
        "# -------------------\n",
        "\n",
        "# Calculate score of model trained with default parameters\n",
        "train_score = logreg.score(X_train, y_train)\n",
        "test_score = logreg.score(X_test, y_test)\n",
        "\n",
        "# Calculate score of model trained with max iterations 5000\n",
        "train_score_max_iter_5000 = logreg_max_iter_5000.score(X_train, y_train)\n",
        "test_score_max_iter_5000 = logreg_max_iter_5000.score(X_test, y_test)\n",
        "\n",
        "# Print the results\n",
        "print(f'With default parameters: \\n\\\n",
        "\\t Train score {train_score:.3}\\n\\\n",
        "\\t Test score {test_score:.3}')\n",
        "print(f'With max_iters = 5000: \\n\\\n",
        "\\t Train score {train_score_max_iter_5000:.3}\\n\\\n",
        "\\t Test score {test_score_max_iter_5000:.3}')\n",
        "\n",
        "# -------------------"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With default parameters: \n",
            "\t Train score 0.909\n",
            "\t Test score 0.808\n",
            "With max_iters = 5000: \n",
            "\t Train score 0.915\n",
            "\t Test score 0.817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3nkZlRHdWs6",
        "colab_type": "text"
      },
      "source": [
        "By addressing that warning messages we managed to get ~1\\% improvement in performance! If you reviewed the  `LogisticRegression` documentation, you may have noticed that the the `fit` method uses L2-regularization by default. Also recall (from the linear regression section) that the features of this model are highly correlated with each other, which is problematic. \n",
        "\n",
        "Let's try using L1-regularization by setting the `penalty` parameter of `LogisticRegression` to 'l1'. We also need to set the solver to 'saga' because the default solver of `LogisticRegression` does not work with L1 regularization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4BXvqiMd-cB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6198e38a-f203-4601-cbf8-f74cb492ca22"
      },
      "source": [
        "# Fit the model\n",
        "logregl1 = LogisticRegression(penalty='l1', solver='saga', max_iter=5000)\n",
        "logregl1.fit(X_train, y_train)\n",
        "\n",
        "# Partition the testing data into features and target\n",
        "X_test = df_test.drop(columns=['W','Team','SeasonEnd', 'Playoffs'])\n",
        "y_test = df_test.Playoffs\n",
        "\n",
        "# calculate the score\n",
        "train_score = logregl1.score(X_train, y_train)\n",
        "test_score = logregl1.score(X_test, y_test)\n",
        "print(f'The train score is {train_score:.3} and the test score is {test_score:.3}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The train score is 0.91 and the test score is 0.808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "R9tvHY5uWv8u",
        "colab_type": "text"
      },
      "source": [
        "## Using cross validation to select regularizer\n",
        "You should see that the L2 regularized model performed better than the L1 regularized model. However, before we make any conclusion about which regularizer is better suited for this problem, it is important to recognize that we did not perform cross-validation to optimize for the strength of the regularizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_xR1fhILsZN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Exercise\n",
        "1. Use the [`LogisticRegressionCV()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html) to fit a model that uses cross-validation and choose the best regularizer strength. Call your model `logregcv`.\n",
        "\n",
        "2. What is the optimal regularization strength? Check the documentation to find what variable it is listed under."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "lr2xqV5IWv8v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "206ca0c6-0d7d-4d13-e65b-92ffaa35c4d8"
      },
      "source": [
        "# Question 1\n",
        "# Write your code here.\n",
        "\n",
        "# -------------------\n",
        "\n",
        "# Fit the model\n",
        "logregcv = LogisticRegressionCV(penalty='l1', solver='saga', max_iter=5000)\n",
        "logregcv.fit(X_train, y_train)\n",
        "\n",
        "# Partition the testing data into features and target\n",
        "X_test = df_test.drop(columns=['W','Team','SeasonEnd', 'Playoffs'])\n",
        "y_test = df_test.Playoffs\n",
        "\n",
        "# calculate the score\n",
        "train_score = logregcv.score(X_train, y_train)\n",
        "test_score = logregcv.score(X_test, y_test)\n",
        "print(f'The train score is {train_score:.3} and the test score is {test_score:.3}')\n",
        "\n",
        "# -------------------"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The train score is 0.903 and the test score is 0.883\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "QRuIk-nsWv8w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e44854f7-b88b-438b-e3b9-06cf8efa6b3a"
      },
      "source": [
        "# Question 2\n",
        "\n",
        "# -------------------\n",
        "\n",
        "print(f'The strength of the regularier is {1/logregcv.C_[0]}')\n",
        "\n",
        "# -------------------"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The strength of the regularier is 10000.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smre2PbJF2Hp",
        "colab_type": "text"
      },
      "source": [
        "## Interpreting model parameters\n",
        "\n",
        "In this section, we'll use the `logregcv` model to make some inferences about making the NBA playoffs. Once again, we can view the coefficients and the intercept of the fitted model using the `coef_` and `intercept_` attributes. However, you may notice that logistic regression has a slightly different format than linear regression! We have provided the code for it below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ITj5y5721KA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "0a55e829-8c7c-4614-dac0-75cc6c2e1d38"
      },
      "source": [
        "# Get beta coefficients\n",
        "betas = pd.Series(logregcv.coef_[0], index=X_train.columns)\n",
        "# Get intercept, and append it to coefficients\n",
        "betas = betas.append(pd.Series({\"Intercept\": logregcv.intercept_[0]}))\n",
        "# Print the all betas (including the intercept)\n",
        "print(betas)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PTS           0.000003\n",
            "oppPTS        0.000000\n",
            "FG            0.000000\n",
            "FGA           0.000000\n",
            "2P            0.000000\n",
            "2PA           0.000000\n",
            "3P            0.000000\n",
            "3PA           0.000000\n",
            "FT            0.000000\n",
            "FTA           0.000000\n",
            "ORB           0.000000\n",
            "DRB           0.000000\n",
            "AST           0.000000\n",
            "STL           0.000000\n",
            "BLK           0.000000\n",
            "TOV           0.000000\n",
            "Conference    0.000000\n",
            "dffPTS        0.007941\n",
            "Intercept     0.000002\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4HBXpmlSibi",
        "colab_type": "text"
      },
      "source": [
        "### Exercise \n",
        "To answer the set of questions in this exercise you'll need to examine the coefficients of the `logregcv` model. You may find the [`predict()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict), [`predict_proba()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba), and [`predict_log_proba()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_log_proba) methods useful.\n",
        "\n",
        "\n",
        "1. Suppose that the Toronto Raptors have the statistics stored in `raptors_2020`. What is the probability that the raptors make the playoffs?\n",
        "\n",
        "2. The Toronto Raptors won the 2019 NBA Championships off of strong defence. Calculate the log-odds and the odds ratio of a team that gets 50 more steals and 100 more blocks. What does the odds ratio say about the likelihood of making the playoffs?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gjKj2MjKDCI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4023e40e-e911-4f21-f2e1-3b0f4acf4a4c"
      },
      "source": [
        "### Question 1\n",
        "\n",
        "raptors_2020 = pd.DataFrame([{'PTS': 10105, 'oppPTS': 10054, 'FG': 3951, \n",
        "                           'FGA':7999, '2P': 3927, '2PA': 7873, '3P': 24,\n",
        "                           \"3PA\": 126, 'FT': 2179, 'FTA': 2696, 'ORB':1214,\n",
        "                           'DRB': 2524, 'AST': 2336, 'STL': 789, 'BLK': 352,\n",
        "                           'TOV': 1496, 'Conference': 0, 'dffPTS': 51}])\n",
        "\n",
        "# Write your code here.\n",
        "\n",
        "# -------------------\n",
        "\n",
        "raps_playoffs_chances = logregcv.predict_proba(raptors_2020)\n",
        "print(f'The raptors have a probability of {raps_playoffs_chances[0, 1]:.3} of making the playoffs.')\n",
        "\n",
        "# -------------------"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The raptors have a probability of 0.607 of making the playoffs.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bj_hsPxUktO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "add8168d-c1bf-4aba-ec1f-ea70146195f2"
      },
      "source": [
        "### Question 2\n",
        "\n",
        "# Write your code here.\n",
        "\n",
        "# -------------------\n",
        "\n",
        "logodds = 50 * betas['STL'] + 100 * betas['BLK']\n",
        "odds = np.exp(logodds)\n",
        "print(f'The log odds ratio is {logodds}, the odds ratio is {odds}')\n",
        "# -------------------"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The log odds ratio is 0.0, the odds ratio is 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lHLnKWDH__Q",
        "colab_type": "text"
      },
      "source": [
        "___\n",
        "**Question 2 Answer:**\n",
        "\n",
        "This implies the chance of making the playoffs is unchanged (no better and no worse). \n",
        "\n",
        "Anyone with some domain knowledge likely finds this result surprising. We observe it because the model has learned that dffPTS is a good feature to predict playoff chances, however, it has not learned what actions are good (e.g., 'STL', 'BLK') to increase dffPTS or playoff chances. A better model to use for this particular problem would be one that uses more features (e.g., `logreg`).\n",
        "___\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb9lXBv-eyze",
        "colab_type": "text"
      },
      "source": [
        "## Classification Error\n",
        "\n",
        "So far, we have only evaluated the model performance based on accuracy, however, it is also good to know what type of errors the model is susceptible to. Scikit-learn provides an easy way to do this via the `confusion_matrix()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCKI3Uo5WfBu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "c26b50e1-50e3-4807-afa4-767f97f010ab"
      },
      "source": [
        "# Make a prediction\n",
        "y_pred = logregcv.predict(X_test)\n",
        "# Make the confusion matrix\n",
        "cfm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
        "# Plot the confusion matrix as a heat map\n",
        "ax = sns.heatmap(cfm, annot=True)\n",
        "# Change the axis lables\n",
        "ax.set(xlabel='Predicted', ylabel='Actual');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RegV0pssHbB",
        "colab_type": "text"
      },
      "source": [
        "### Exercise\n",
        "Use the Confusion matrix above to answer the following questions.\n",
        "\n",
        "1. What is the number of true positives? true negatives?\n",
        "2. What is the biggest type of error that the model makes?\n",
        "3. Calculate the recall and precision. \n",
        "4. Given these scores, if a team were to make the playoffs in reality, how likely do you think that the model would predict correctly? \n",
        "5. If the model predicts that a team will make the playoffs, how likely do you think that the model is correct?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in-jX0u6Wgx0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25504316-dc8c-4adf-cffb-6f7635150601"
      },
      "source": [
        "### Question 1\n",
        "\n",
        "# Write your code here.\n",
        "\n",
        "# -------------------\n",
        "\n",
        "true_negatives = cfm[0,0]\n",
        "true_positives = cfm[1,1]\n",
        "print(f'There are {true_negatives} true negatives and {true_positives} true positives')\n",
        "\n",
        "# -------------------"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 51 true negatives and 55 true positives\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP3g3SnWs4xd",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "___\n",
        "**Question 2 answer:** False positives\n",
        "___\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqfGd9Ozcj2o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30a1bc2c-3bd6-4448-ca30-fba4b9d3a33c"
      },
      "source": [
        "### Question 3\n",
        "\n",
        "# Write your code here.\n",
        "\n",
        "# -------------------\n",
        "\n",
        "recall = cfm[1,1] / (cfm[1,0] + cfm[1,1])\n",
        "precision = cfm[1, 1] / (cfm[0, 1] + cfm[1, 1])\n",
        "print(f'The recall is {recall} and the precision is {precision}')\n",
        "\n",
        "# -------------------"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The recall is 0.8870967741935484 and the precision is 0.8870967741935484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUom9oy3uiNG",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "___\n",
        "**Question 4 answer:**\n",
        "\n",
        "If a team were to make it (i.e., Actual = 1), then we predict it accurately nearly always. (Recall)\n",
        "\n",
        "___\n",
        "\n",
        "\n",
        "___\n",
        "**Question 5 Answer:**\n",
        "\n",
        "If a team were to make it (i.e., Actual = 1), then we predict it accurately nearly always. (Precision)\n",
        "\n",
        "___\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfDhUZcqIfWb",
        "colab_type": "text"
      },
      "source": [
        "## Picking a prediction threshold\n",
        "Recall that a logistic regression model outputs the probability of whether the the target is 1 or 0. We then apply a threshold rule to turn that probability into a prediction. The default threshold in scikit-learn is 0.5, but it is a good idea to examine the raw probabilities to see if a better threshold exists.\n",
        "\n",
        "The tool that is most used to help choose a threshold is a receiver operating characteristic (ROC) curve. The ROC curve is a plot of true positive rate (TPR) versus false positive rate (FPR) for different threshold values. It is used in a lot in applications where it is important to make sure that there are very few false positives (i.e., false alarms) or very few false negatives (i.e., missed detections).\n",
        "\n",
        "In this section you will plot ROC curves for the three logistic regression models that we built (i.e., `logreg`, `logregl1`, `logregcv`). The [`roc_curve()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) function in `sklearn` makes it easy to generate the data for an ROC curve plot. As input, this function takes the ground truth target label and the model's estimated probability of \"1\", which is generated by the [`predict_proba()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba) method. As output, `roc_curve()` returns the FPR, TPR, and the threshold to achieve those statistics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8UViU-pyvMx",
        "colab_type": "text"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "1. Plot FPR and TPR using [`sns.lineplot()`](https://seaborn.pydata.org/generated/seaborn.lineplot.html) to generate an ROC curve for each of the three models (i.e., `logreg`, `logregl1`, `logregcv`). We have provided some of the code to format the resulting plot.\n",
        "\n",
        "2. What does the straight line from (0,0) to (1,1) represent?\n",
        "\n",
        "3. According to the `roc_curve()` function, what is the highest threshold value at which you should get a TPR=1 for your L1-cross-validated model?\n",
        "\n",
        "4. Is there any scenario where the cross-validated model is worse than any of the other models?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKQ58mHkumIV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "57b85374-0075-493c-a1f0-5a7708878dac"
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# Write your code here.\n",
        "\n",
        "# -------------------\n",
        "\n",
        "# L2 model\n",
        "ylogreg = logreg.predict_proba(X_test).T[1]  # Predictions without threshold \n",
        "fprlogreg, tprlogreg, threshlogreg = roc_curve(y_test, ylogreg)  \n",
        "\n",
        "# L1 model\n",
        "ylogregl1 = logregl1.predict_proba(X_test).T[1]\n",
        "fprlogregl1, tprlogregl1, threshlogregl1 = roc_curve(y_test, ylogregl1)\n",
        "\n",
        "# L1 model with CV\n",
        "ylogregcv = logregcv.predict_proba(X_test).T[1]\n",
        "fprlogregcv, tprlogregcv, threshlogregcv = roc_curve(y_test, ylogregcv)\n",
        "\n",
        "# Plot the ROC of each model with approporiate lables for legend\n",
        "sns.lineplot(fprlogreg, tprlogreg, label='Original')\n",
        "sns.lineplot(fprlogregl1, tprlogregl1, label='L1')\n",
        "sns.lineplot(fprlogregcv, tprlogregcv, label='L1-CV')\n",
        "\n",
        "# -------------------\n",
        "\n",
        "# Plot format\n",
        "sns.lineplot([0, 1], [0, 1], linestyle='--')  # add red line\n",
        "plt.xlim([0.0, 1.0])  # limit x-axis between 0 and 1\n",
        "plt.ylim([0.0, 1.0])  # limit y-axis between 0 and 1\n",
        "plt.xlabel('False Positive Rate')  # lable for x axis\n",
        "plt.ylabel('True Positive Rate')  # lable for y axis\n",
        "plt.legend(loc='lower right')  # print legend in lower right corner\n",
        "plt.show()  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izHwsKktTklo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "9af0cb1b-aa2f-4721-f2a8-12b1fe6c4c86"
      },
      "source": [
        "threshlogregl1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.99999999e+00, 9.99999991e-01, 9.99871187e-01, 9.99851937e-01,\n",
              "       9.99816306e-01, 9.99778719e-01, 9.99222278e-01, 9.99008518e-01,\n",
              "       9.98443082e-01, 9.98310013e-01, 9.94928179e-01, 9.94860173e-01,\n",
              "       9.94131107e-01, 9.93300796e-01, 9.81291603e-01, 9.80163602e-01,\n",
              "       9.78153043e-01, 9.76005163e-01, 9.66713898e-01, 9.58771002e-01,\n",
              "       9.53306431e-01, 9.49773877e-01, 9.39164414e-01, 9.24068772e-01,\n",
              "       9.22578115e-01, 8.87235640e-01, 8.79800360e-01, 8.47063399e-01,\n",
              "       8.31381922e-01, 7.95228378e-01, 7.63353358e-01, 7.46495526e-01,\n",
              "       7.10173352e-01, 6.52543498e-01, 5.35521503e-01, 5.22519041e-01,\n",
              "       4.04966064e-01, 2.56262685e-01, 1.97142828e-01, 1.25453023e-05])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEulLyjMvciD",
        "colab_type": "text"
      },
      "source": [
        "___\n",
        "**Question 2 answer:**\n",
        "\n",
        "If you were to randomly guess 1 or 0 without using any features\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbFCuPtFwOcp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cb016c29-0c2d-4210-e4cf-c9f3c79aaaf2"
      },
      "source": [
        "# Question 3\n",
        "\n",
        "# Write your code here.\n",
        "\n",
        "# -------------------\n",
        "first_1_in_TPR = np.argwhere(tprlogregcv==1)[0,0]  # Return first instance of 1\n",
        "highest_TPR_1_threshold = threshlogregcv[first_1_in_TPR]\n",
        "print(f'The highest threshold that achieves 100% TPR is {highest_TPR_1_threshold:.3}')\n",
        "# -------------------"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The highest threshold that achieves 100% TPR is 0.192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9IrUcxp0rca",
        "colab_type": "text"
      },
      "source": [
        "___\n",
        "**Question 4 answer**:\n",
        "\n",
        "No. In general, a curve is worse than another curve if it is closer to the red line.\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfp-CzkCkBvp",
        "colab_type": "text"
      },
      "source": [
        "# Reflection\n",
        " If we want to build a prediction model it is critical that we have access to its features at a reasonable time in real life. Unfortunately, in our case the models in this lab are not very useful because they use the features of a season (e.g., how many points a team scored) to predict outcomes for that same season (e.g., whether or not a team made playoffs) only after those outcomes are effectively known. A more impactful predictive model would use the features at the end of the season to predict if a team will make playoffs in the following season (e.g., use 2015 data to predict `'Playoffs'` in 2016). In this section you're going to code up that very model!\n",
        "\n",
        "In the code block below, we do some (advanced) data processing to prepare the data you need to predict whether or not a team will qualify for playoffs next year. We use the column `'NextYearPlayoffs'` to denote whether or not a team will (`'NextYearPlayoffs'` = 1) or will not (`'NextYearPlayoffs'` = 0) make playoffs. The code block below generates this feature using [multi indexing](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html), but we don't expect you to understand multi-indexing at this stage in the course  (i.e., the code block immediately below will not come up in any capacity during Quiz 1).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4up7PQA0muda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"In this codeblock, we process the data for you using functions that you DO \n",
        "NOT need to be familiar with right now (you won't be tested on this type of \n",
        "data manipulation in Quiz 1). Please focus on the logic behind this code block\n",
        "and understand why we modify the data the way we do.\"\"\"\n",
        "\n",
        "# Make a copy of the original DataFrame\n",
        "X = df.copy()\n",
        "\n",
        "# Change the index to Team and Season\n",
        "X = X.set_index(['Team', 'SeasonEnd'])\n",
        "# Keep SeasonEnd as one of the columns\n",
        "X['SeasonEnd'] = X.index.get_level_values(1).values \n",
        "\n",
        "# Get y data (i.e., if they make playoffs next year)\n",
        "y = pd.concat((df['Team'], \n",
        "               df['SeasonEnd']-1, \n",
        "               df['Playoffs'].rename('NextYearPlayoffs')\n",
        "              ), \n",
        "              axis=1)\n",
        "# Set the index to the same convention as X\n",
        "y = y.set_index(['Team', 'SeasonEnd'])\n",
        "\n",
        "# Get intersection of indicies between X and y\n",
        "index_intersection = X.index.intersection(y.index)\n",
        "X = X.loc[index_intersection]  # Season stats\n",
        "y = y.loc[index_intersection]  # Whether or no team makes playoffs next year"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cacqc_P3XUd",
        "colab_type": "text"
      },
      "source": [
        "### Exercise\n",
        "1. Use the newly processed data (X and y) to predict whether or not a team will make playoffs in the following year based on their stats from the previous season.\n",
        "\n",
        "2. Based on mean accuracy, you should see that this new model performs worse than the previous models. Why do you think that is?\n",
        "\n",
        "3. In what way is the first set of models useful (i.e., predicting playoffs based on this season's data), and in which way is this new model useful (i.e., predicting playoffs based off last season's data)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMEDUJbv3Xla",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8ce982a-d043-4848-fa09-2526f4774875"
      },
      "source": [
        "# Question 1\n",
        "# Write your code here.\n",
        "\n",
        "# -------------------\n",
        "# Split into training and testing\n",
        "X_train = X[X.SeasonEnd<2012]\n",
        "y_train = y.loc[X.SeasonEnd<2012].NextYearPlayoffs\n",
        "X_test = X[X.SeasonEnd>2012]\n",
        "y_test = y[X.SeasonEnd>2012].NextYearPlayoffs\n",
        "\n",
        "# Fit the model\n",
        "logregpred = LogisticRegressionCV(penalty='l1', solver='saga', max_iter=5000)\n",
        "logregpred.fit(X_train, y_train)\n",
        "\n",
        "# Partition the testing data into features and target\n",
        "mdl_score = logregpred.score(X_test, y_test)\n",
        "\n",
        "print(mdl_score)\n",
        "# -------------------\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6888888888888889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OVYrQ8UU8fR",
        "colab_type": "text"
      },
      "source": [
        "___\n",
        "**Question 2 answer:**\n",
        "\n",
        "This model performed worse because it uses features from a previous season, which don't directly affect a team's chances of making playoffs. There is no guarantee that a team can repeat those statistics in the following year. In contrast, the earlier models in this lab predicted whether or not a team made playoffs only when they knew __exactly__ how a team performed (i.e., had perfect information).\n",
        "___\n",
        "**Question 3 answer:**\n",
        "\n",
        "The previous model is useful for identifying features that lead to new playing/practice strategies, which can inform coaching decisions. In contrast, the new model is useful for understanding how a team will do in the following season if they keep things the same (e.g., player roster) for next year, which can inform manger decisions (e.g., player trades).\n",
        "___\n"
      ]
    }
  ]
}